% Notes and exercises from Linear Algebra and Geometry by Dieudonne
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier}

\newcommand{\R}{\mathbf{R}}
\newcommand{\Rnz}{\R^*}

\newcommand{\iso}{\cong}

\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\after}{\circ}
\newcommand{\mult}{\cdot}

\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{\mathbf{GL}}
\DeclareMathOperator{\GA}{\mathbf{GA}}
\DeclareMathOperator{\rank}{rank}

\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\kerz}[1]{\inv{#1}(0)}
\newcommand{\dual}[1]{#1^*}

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra and Geometry}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

\section*{Introduction}
This document contains notes and exercises from~\cite{dieudonne}.

\section*{Chapter~III}
\subsection*{Section~1}
\begin{exer}[4]
Let \(V,W\) be a pair of supplementary subspaces of~\(E\). Every subspace~\(U\) containing~\(V\) is the direct sum of~\(V\) with~\(U\sect W\).
\end{exer}
\begin{proof}
If \(u\in U\), then \(u=v+w\) for some \(v\in V\) and \(w\in W\), and \(w=u-v\in U\). So \(U=V+(U\sect W)\), and \(V\sect U\sect W=\{0\}\).
\end{proof}

\subsection*{Section~2}
\begin{exer}[1]
\emph{Projections and idempotents:} If \(p,q\)~are the projections for a direct sum \(E=V+W\), then \(p,q\in\End(E)\) are such that \(p^2=p\), \(q^2=q\), and \(p+q=1\). Conversely, if \(p\in\End(E)\) is such that \(p^2=p\), then \(E=p(E)+\kerz{p}\) is a direct sum. Moreover, if \(q=1-p\), then \(q^2=q\), \(q(E)=\kerz{p}\), and \(\kerz{q}=p(E)\).
\end{exer}
\begin{proof}
For the forward direction, we know \(p,q\in\End(E)\) and \(p+q=1\) (3.2.2). It follows that \(p^2=p\after(1-q)=p-pq=p\) and \(q^2=(1-p)^2=1-p=q\).

For the converse, \(p+q=1\), so \(E=p(E)+q(E)\). Also \(pq=p-p^2=0\), so \(p(E)\sect q(E)=\{0\}\) and \(q(E)\subseteq\kerz{p}\). If \(x\in\kerz{p}\), then \(q(x)=x\), so \(x\in q(E)\). Hence \(q(E)=\kerz{p}\) and similarly \(\kerz{q}=p(E)\). Finally \(q^2=q\) as above.
\end{proof}

\begin{exer}[2]
If \(W\) and~\(W'\) are both supplementary to~\(V\) in~\(E\), then \(W\) and~\(W'\) are isomorphic.
\end{exer}
\begin{proof}
If \(p\)~is the projection of~\(E\) onto~\(W'\), then the restriction of~\(p\) to~\(W\) is an isomorphism from~\(W\) to~\(W'\).
\end{proof}

\begin{exer}[3]
If \(E=V+W\) is a direct sum with inclusions \(i:V\to E\) and \(j:W\to E\), and \(v:V\to F\) and \(w:W\to F\) are linear maps, then there is a unique linear map \(u:E\to F\) with \(u\after i=v\) and \(u\after j=w\).
\end{exer}
\begin{proof}
If \(p,q\) are the projections on \(V,W\) respectively, then \(u=v\after p+w\after q\).
\end{proof}

\begin{exer}[11]
\(\GA(E)/E\iso\GL(E)\).
\end{exer}
\begin{proof}
Define \(\varphi:\GA(E)\to\GL(E)\) by \(\varphi(t_a\after v)=v\). Note that \(\varphi\)~is well-defined by~(3.2.17), \(\varphi\)~is a homomorphism by~(3.2.19), and \(\varphi\)~is clearly surjective. Also \(\varphi(u)=1\) if and only if \(u\)~is a translation, so \(\ker\varphi=T(E)\), the normal subgroup of translations. It follows that \(\GA(E)/T(E)\iso\GL(E)\). Finally, the mapping \(a\mapsto t_a\) is an isomorphism \(E\iso T(E)\) from the additive group~\(E\).
\end{proof}

\begin{exer}[13]
If \(u:E\to F\) is affine and \(L\)~is a variety in~\(F\), then \(\inv{u}(L)\)~is empty or a variety in~\(E\).
\end{exer}
\begin{proof}
If \(a\in\inv{u}(L)\) and \(L_0\)~is the direction of~\(L\), then \(L=u(a)+L_0\) and hence \(\inv{u}(L)=a+\inv{u}(L_0)\).
\end{proof}

\subsection*{Section~3}
\begin{exer}[3]
A necessary and sufficient condition for a nonempty subset~\(V\) of a vector space to be a variety is that for all pairs~\(x,y\) of distinct points of~\(V\), the line~\(D_{xy}\) is contained in~\(V\).
\end{exer}
\begin{proof}
The condition is necessary by~(3.3.2).

If the condition holds, choose \(v\in V\) and let \(V_0=-v+V\). We claim \(V_0\)~is a subspace, from which it follows that \(V=v+V_0\) is a variety. First, \(0=-v+v\in V_0\). If \(x\in V_0\) and \(x\ne 0\), then \(v+x\in V\) and \(v+x\ne v\), so \(D_{v,v+x}=\{\,v+\xi x\mid\xi\in\R\,\}\subseteq V\). It follows that \(\xi x\in V_0\) for all \(\xi\in\R\). If also \(y\in V_0\) and \(y\ne x\), then \(D_{v+x,v+y}\subseteq V\), so in particular \(v+2^{-1}(x+y)\in V\) and \(2^{-1}(x+y)\in V_0\). By the previous result, it then follows that \(x+y\in V_0\). Therefore \(V_0\)~is a subspace as claimed.
\end{proof}

\goodbreak
\begin{exer}[4]
\emph{Translations and homothetic maps:}
\begin{itemize}[itemsep=0pt]
\item A necessary and sufficient condition for an affine map to be a translation or a homothetic map is that its associated linear map be homothetic.
\item A necessary and sufficient condition for an affine map to preserve the direction of lines is that it be a translation or a bijective homothetic map.
\item If \(u_1,u_2\)~are translations or homothetic maps, then so is~\(u_1\after u_2\).
\item If \(u_1,u_2\) and~\(u_1\after u_2\) are homothetic maps with ratios not equal to 1, their centers are collinear. If instead \(u_1\after u_2\)~is a translation, then it is either the identity or a translation in the direction of the line through the centers of \(u_1\) and~\(u_2\).
\item If \(v\in\GA(E)\), then \(v\after h_{a,\lambda}\after\inv{v}=h_{v(a),\lambda}\).
\item The subset~\(H(E)\) of translations and homothetic maps in~\(\GA(E)\) forms a subgroup, and \(H(E)/E\iso\Rnz\).
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item This follows from the equations \(t_a=t_a\after h_1\) and \(h_{a,\lambda}=t_{(1-\lambda)a}\after h_{\lambda}\) and \(t_a\after h_{\lambda}=h_{(1-\lambda)^{-1}a,\lambda}\) (\(\lambda\ne 1\)).

\item The condition is sufficient because such a map has the form \(t_a\after h_{\lambda}\) with \(\lambda\ne 0\), which clearly preserves the direction of lines. Conversely, suppose \(u=t_a\after v\) preserves the direction of lines. If \(x\ne 0\), let \(D\)~be the vector line through~\(x\). Then \(v(D)=D\), so \(v(x)=\lambda x\) for some \(\lambda\in\R\) with \(\lambda\ne0\), and in fact \(v(y)=\lambda y\) for all \(y\in D\). We claim \(v=h_{\lambda}\), from which the result follows. If \(y\not\in D\), then by considering the vector line~\(D'\) through~\(y\) we have \(v(y)=\mu y\) for some \(\mu\in\R\). Now \(v(D_{xy})=D_{v(x)v(y)}=D_{\lambda x,\mu y}\), and since \(v\)~preserves direction there is \(\xi\in\R\) with \(\mu y-\lambda x=\xi(y-x)\), or \((\mu-\xi)y=(\lambda-\xi)x\). Since \(y\not\in D\), this implies \(\mu=\xi=\lambda\). Therefore \(v=h_{\lambda}\) as claimed.

\item If \(u_1=t_a\after h_{\lambda}\) and \(u_2=t_b\after h_{\mu}\), then by~(3.2.19.1), \(u_1\after u_2=t_{a+\lambda b}\after h_{\lambda\mu}\).

\item Write \(u_1=h_{a,\lambda}\), \(u_2=h_{b,\mu}\), and \(u_1\after u_2=h_{c,\nu}\) with \(\lambda,\mu,\nu\ne1\). Then for all~\(x\),
\[(1-\nu)c+\nu x=(1-\lambda)a+\lambda(1-\mu)b+\lambda\mu x\]
Taking \(x=0\) and \(x\ne0\) (we assume such exist!) yields \(\nu=\lambda\mu\) and
\[c=(1-\lambda\mu)^{-1}[(1-\lambda)a+\lambda(1-\mu)b]=(1-\lambda\mu)^{-1}(\lambda-1)(b-a)+b\]
so that \(a,b,c\) are collinear. If instead \(u_1\after u_2=t_c\), then \(\lambda\mu=1\) and \(c=(\lambda-1)(b-a)\), from which the second result follows.

\item Write \(v=t_b\after w\), where \(w\in\GL(E)\), so that \(\inv{v}=\inv{w}\after t_{-b}\). By repeated application of~(3.2.19.1),
\begin{align*}
v\after h_{a,\lambda}\after\inv{v}&=t_b\after w\after t_{(1-\lambda)a}\after h_{\lambda}\after\inv{w}\after t_{-b}\\
	&=t_{v((1-\lambda)a)}\after w\after h_{\lambda}\after\inv{w}\after t_{-b}\\
	&=t_{b+(1-\lambda)w(a)}\after h_{\lambda}\after t_{-b}\\
	&=t_{b+(1-\lambda)w(a)-\lambda b}\after h_{\lambda}\\
	&=t_{(1-\lambda)v(a)}\after h_{\lambda}\\
	&=h_{v(a),\lambda}
\end{align*}

\item First, \(1\in H(E)\). If \(u_1,u_2\in H(E)\), then \(u_1\after u_2\in H(E)\) by a previous item. If \(u_1=t_a\after h_{\lambda}\) with \(\lambda\ne0\), then \(\inv{u_1}=t_{-\lambda^{-1}a}\after h_{\lambda^{-1}}\in H(E)\). Therefore \(H(E)\)~is a subgroup of~\(\GA(E)\). Define \(\varphi:H(E)\to\Rnz\) by \(\varphi(t_a\after h_{\lambda})=\lambda\). Note that \(\varphi\)~is well-defined (since \(E\ne\{0\}\)!), \(\varphi\)~is a homomorphism by a previous item, \(\varphi\)~is surjective, and \(\ker\varphi=T(E)\iso E\). It follows that \(H(E)/E\iso\Rnz\).\qedhere
\end{itemize}
\end{proof}

\begin{exer}[5]
A variety not parallel to a hyperplane meets the hyperplane. If \(H\)~is a vector hyperplane in~\(E\) and \(V\)~is a subspace of~\(E\) not contained in~\(H\), then \(V\sect H\)~is a vector hyperplane in~\(V\).
\end{exer}
\begin{proof}
Any vector in the direction of the variety which is not in the direction of the hyperplane determines a line in the variety which meets the hyperplane (3.3.8).

If \(D\)~is the vector line through a vector in \(V-H\), then \(D\)~is supplementary to~\(H\) in~\(E\), so \(D\)~is supplementary to~\(V\sect H\) in~\(V\) (Exercise~3.1.4).
\end{proof}

\begin{exer}[6]
\emph{Dilations and transvections:} Let \(H\)~be a vector hyperplane in~\(E\) and suppose \(u\in\End(E)\) fixes every element of~\(H\).
\begin{itemize}[itemsep=0pt]
\item There is \(\gamma\in\R\) unique such that \(u(a)\in\gamma a+H\) for all \(a\in E-H\).
\item If \(\gamma\ne 1\), then \(\gamma\)~is an eigenvalue of~\(u\) and \(E(\gamma;u)\)~is a line~\(S\) supplementary to \(E(1;u)=H\). A subspace~\(V\) satisfies \(u(V)\subseteq V\) if and only if \(S\subseteq V\) or \(V\subseteq H\). In particular, a vector line~\(D\) satisfies \(u(D)\subseteq D\) if and only if \(D=S\) or \(D\subseteq H\).
\item If \(\gamma=1\), and \(g(x)=0\) is an equation of~\(H\), there is \(c\in H\) unique such that \(u(x)=x+g(x)c\) for all \(x\in E\). \(u\)~is bijective. If \(u\ne 1\) (so \(c\ne 0\)), then the line \(T=D_{0c}\) is independent of~\(g\). The scalar~\(1\) is the only eigenvalue of~\(u\) if \(H\ne\{0\}\), and \(E(1;u)=H\) if \(u\ne 1\). If \(u\ne 1\), a subspace~\(V\) satisfies \(u(V)\subseteq V\) if and only if \(T\subseteq V\) or \(V\subseteq H\); in particular, a vector line~\(D\) satisfies \(u(D)=D\) if and only if \(D\subseteq H\).
\item The set~\(\Gamma(E,H)\) of automorphisms of~\(E\) leaving the hyperplane~\(H\) fixed pointwise is a subgroup of~\(\GL(E)\). The subset~\(\Theta(E,H)\) of transvections is a normal abelian subgroup of~\(\Gamma(E,H)\) isomorphic to~\(H\). \(\Gamma(E,H)/H\iso\Rnz\).
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item If \(a\in E-H\), then \(E=\R a+H\), so \(u(a)=\gamma a+t\) for some \(\gamma\in\R\) and \(t\in H\). If \(b\in E\), then \(b=\beta a+h\) for some \(\beta\in\R\) and \(h\in H\), so
\begin{align*}
u(b)&=u(\beta a+h)\\
	&=\beta u(a)+u(h)\\
	&=\beta(\gamma a+t)+h\\
	&=\gamma(\beta a+h)+(1-\gamma)h+\beta t\\
	&=\gamma b+(1-\gamma)h+\beta t\tag{1}
\end{align*}
Therefore \(u(b)\in\gamma b+H\). If also \(u(b)\in\gamma'b+H\), then \((\gamma'-\gamma)b\in H\), which implies \(\gamma'=\gamma\) if \(b\not\in H\). Therefore \(\gamma\)~is unique for \(b\not\in H\).

\item Let \(x=a-(1-\gamma)^{-1}t\). Then \(x\ne 0\) since \(a\not\in H\) and
\begin{align*}
u(x)&=u(a-(1-\gamma)^{-1}t)\\
	&=u(a)-(1-\gamma)^{-1}u(t)\\
	&=\gamma a+t-(1-\gamma)^{-1}t\\
	&=\gamma[a-(1-\gamma)^{-1}t]\\
	&=\gamma x
\end{align*}
Therefore \(x\)~is an eigenvector of~\(u\) with eigenvalue~\(\gamma\). Let \(S\)~be the vector line through~\(x\). Then \(S\subseteq E(\gamma;u)\). Conversely if \(b\in E(\gamma;u)\), then \(u(b)=\gamma b\), which implies \((1-\gamma)h+\beta t=0\) in~(1), so \(h=-\beta(1-\gamma)^{-1}t\) and
\[b=\beta a+h=\beta[a-(1-\gamma)^{-1}t]=\beta x\in S\]
Therefore \(S=E(\gamma;u)\). By hypothesis \(H\subseteq E(1;u)\). Conversely if \(b\in E(1;u)\), then \(b=u(b)\in\gamma b+H\), so \((1-\gamma)b\in H\), so \(b\in H\). Therefore \(H=E(1;u)\). \(S\)~is supplementary to~\(H\) since \(x\not\in H\).

By hypothesis \(u(V)\subseteq V\) for any subspace \(V\subseteq H\). If \(S\subseteq V\), then \(V=S+V\sect H\) (Exercise~3.1.4), so clearly \(u(V)\subseteq V\). Conversely if \(u(V)\subseteq V\) and \(v\in V-H\), then \(v=s+h\) for some \(s\in S\) with \(s\ne 0\) and \(h\in H\), so \(u(v)=\gamma s+h\) and \(v-u(v)=(1-\gamma)s\in V\), which implies \(s\in V\) and \(S=\R s\subseteq V\).

\item Fix \(e\in E\) with \(g(e)=1\) and let \(c=u(e)-e\). Since \(u(e)\in e+H\), \(g(c)=0\) and \(c\in H\). Now \(u(x)=x+g(x)c\) holds for \(x=e\), and for \(x\in H\), so by linearity it holds for all \(x\in \R e+H=E\). Note \(c\)~is unique since if \(u(e)=e+g(e)c'\), then \(c'=u(e)-e=c\).

The map \(x\mapsto x-g(x)c\) is clearly the inverse of~\(u\), so \(u\)~is bijective.

If \(h(x)=0\) is another equation of~\(H\), then by the above there exists \(c'\in H\) such that \(u(x)=x+h(x)c'\) for all \(x\in E\). But \(h=\lambda g\) for some \(\lambda\ne 0\) (3.3.6), so \(u(x)=x+g(x)(\lambda c')\) for all \(x\in E\) and \(c=\lambda c'\) by uniqueness of~\(c\). If \(u\ne 1\), then \(T=D_{0c}=D_{0c'}\) is independent of~\(g\).

If \(u(x)=x+g(x)c=\lambda x\), then \((\lambda-1)x\in H\). If \(\lambda\ne 1\), then \(x\in H\), so actually \((\lambda-1)x=0\) and \(x=0\). If \(\lambda=1\), then \(g(x)c=0\), so either \(g(x)=0\) and \(x\in H\), or \(c=0\) and \(u=1\).

As above, \(u(V)\subseteq V\) if \(V\subseteq H\) or \(T\subseteq V\). Conversely if \(u(V)\subseteq V\) and \(x\in V-H\), then \(g(x)\ne 0\) and \(g(x)c=u(x)-x\in V\), so \(c\in V\) and \(T=\R c\subseteq V\).\qedhere

\item \(\Gamma(E,H)\)~is obviously a subgroup of~\(\GL(E)\). Define \(\varphi:\Gamma(E,H)\to\Rnz\) by \(\varphi(u)=\gamma\). Then \(\varphi\)~is a well-defined, surjective homomorphism and \(\ker\varphi=\Theta(E,H)\). It follows that \(\Theta(E,H)\)~is a normal subgroup of~\(\Gamma(E,H)\) and that \(\Gamma(E,H)/\Theta(E,H)\iso\Rnz\). Finally, the mapping \(c\mapsto(x\mapsto x+g(x)c)\) is an isomorphism \(H\iso\Theta(E,H)\), so in particular \(\Theta(E,H)\)~is abelian.
\end{itemize}
\end{proof}

\section*{Chapter~IV}
\subsection*{Section~1}
\begin{exer}[1]
If \(t\ne1\) is a transvection in~\(E\), a basis for~\(E\) may be chosen relative to which
\[M(t)=\begin{pmatrix}
1&1\\
0&1
\end{pmatrix}\]
Conversely, matrices of the form
\[B_{12}(\lambda)=\begin{pmatrix}
1&\lambda\\
0&1
\end{pmatrix}
\qquad
B_{21}(\lambda)=\begin{pmatrix}
1&0\\
\lambda&1
\end{pmatrix}\]
are matrices of transvactions relative to some basis~\(\{a_1,a_2\}\). These transvections have the lines \(D_{0a_1}\) and~\(D_{0a_2}\) respectively.
\end{exer}
\begin{proof}
Write \(t(x)=x+g(x)c\), where \(g\in\dual{E}\) with \(g\ne 0\), and \(c\ne 0\) with \(g(c)=0\) (Exercise~3.3.6). Let \(a_1=c\) and choose~\(a_2\) such that \(g(a_2)=1\). Then \(\{a_1,a_2\}\)~is the desired basis for~\(E\).

Conversely, if \(t\)~is the transformation of~\(B_{12}(\lambda)\) relative to~\(\{a_1,a_2\}\), then
\begin{align*}
t(x)&=t(\xi_1a_1+\xi_2a_2)\\
	&=\xi_1 t(a_1)+\xi_2 t(a_2)\\
	&=\xi_1 a_1+\xi_2(\lambda a_1+a_2)\\
	&=\xi_1 a_1+\xi_2 a_2+\lambda\xi_2 a_1\\
	&=x+g(x)a_1
\end{align*}
where \(g=\lambda\dual{a_2}\in\dual{E}\). Therefore \(t\)~is a transvection in the line~\(D_{0a_1}\). A similar argument applies to~\(B_{21}(\lambda)\).
\end{proof}

\begin{exer}[3]
If \(u\in\End(E)\) and \(\rank(u)=1\), there is a basis of~\(E\) relative to which
\[M(u)=\begin{pmatrix}
\delta&0\\
0&0
\end{pmatrix}
\quad(\delta\ne 0)
\qquad\text{or}\qquad
M(u)=\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}\]
The second case occurs if and only if \(u\)~is nilpotent, in which case \(u^2=0\).
\end{exer}
\begin{proof}
Let \(N=\kerz{u}\) and \(R=u(E)\). Then \(N\) and~\(R\) are vector lines in~\(E\) (4.1.7). If \(N\sect R=\{0\}\), then \(E=N+R\) is a direct sum. Choose \(a_1\in R\) and \(a_2\in N\) with \(a_1,a_2\ne 0\). Then \(u(a_1)=\delta a_1\) with \(\delta\ne 0\) since \(u(a_1)\in R\) and \(a_1\not\in N\), and also \(u(a_2)=0\). It follows that \(\{a_1,a_2\}\)~is a basis of~\(E\) relative to which
\[M(u)=\begin{pmatrix}
\delta&0\\
0&0
\end{pmatrix}\]
If \(N\sect R\ne\{0\}\), then \(N=R\) (3.3.1). Choose \(a_1\in N\), \(a_1\ne 0\) and \(a_2\)~with \(u(a_2)=a_1\). Then \(\{a_1,a_2\}\)~is a basis of~\(E\) (since \(a_2\not\in N\)) relative to which
\[M(u)=\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}\]
If \(M(u)\)~has this form, then \(M(u^2)=M(u)^2=0\), so \(u^2=0\) and \(u\)~is nilpotent. Conversely if \(u\)~is nilpotent, there is \(k>1\) least such that \(u^k=0\). Then \(u^{k-1}(E)\ne 0\) but \(u^{k-1}(E)\subseteq N\), so \(u^{k-1}(E)=N\). If \(k>2\), then \(u^{k-1}(E)\)~is a proper subspace of~\(u^{k-2}(E)\), lest \(u^{k-2}(E)=N\) and \(u^{k-1}=0\). But then \(u^{k-2}(E)=E\), impossible since \(\rank(u)=1\). It follows that \(u^2=0\), so \(N=R\) and \(M(u)\)~has this form.
\end{proof}

\begin{exer}[7]
Let \(u:E\to E\) be an injective function such that \(E\)~is the smallest variety containing~\(u(E)\), and \(u(a),u(b),u(c)\) are collinear whenever \(a,b,c\in E\) are collinear.
\begin{itemize}[itemsep=0pt]
\item If \(a,b,c\in E\) are not collinear, then \(u(a),u(b),u(c)\) are not collinear.
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item Suppose towards a contradiction that \(u(a),u(b),u(c)\) are on the line~\(\Delta\). Let \(x\in E\). If \(x\in D_{ab}\union D_{ac}\union D_{bc}\), then \(u(x)\in\Delta\). Otherwise if, say, \(x\) and~\(c\) are on opposite sides of~\(D_{ab}\), then \(D_{xc}\) and~\(D_{ab}\) intersect at a unique point~\(y\) by (3.3.9) and~(4.1.6), \(u(y)\in\Delta\) since \(y\in D_{ab}\), and \(u(x)\in\Delta\) since \(x\in D_{yc}\). Similarly \(u(x)\in\Delta\) if \(x\) and~\(b\) are on opposite sides of~\(D_{ac}\), or \(x\) and~\(a\) are on opposite sides of~\(D_{bc}\). Finally, if none of these cases hold, then \(D_{xc}\)~cannot be parallel to~\(D_{ab}\), because if \(x\)~is in the direction of~\(a-b\) from~\(c\) (that is, if \(x=c+\xi(a-b)\) for \(\xi>0\)) then \(x\) and~\(b\) are on opposite sides of~\(D_{ac}\), and if \(x\)~is in the direction of~\(b-a\) from~\(c\) then \(x\) and~\(a\) are on opposite sides of~\(D_{bc}\). Therefore \(D_{xc}\) and~\(D_{ab}\) intersect at a unique point~\(y\) by~(4.1.6) and \(u(x)\in\Delta\) as above. Since \(x\)~was arbitrary, this means \(u(E)\subseteq\Delta\), contradicting the hypothesis about~\(u(E)\).
\end{itemize}
\end{proof}

\begin{exer}[8]
We have
\[\begin{pmatrix}
\alpha_1\\
\alpha_2
\end{pmatrix}
\begin{pmatrix}
\beta_1&\beta_2
\end{pmatrix}
=
\begin{pmatrix}
\alpha_1\beta_1&\alpha_1\beta_2\\
\alpha_2\beta_1&\alpha_2\beta_2
\end{pmatrix}\]
This can be interpreted as the composite of a linear form \(E\to\R\) and a linear map \(\R\to E\); it maps the plane onto the origin or onto a vector line and hence is of rank \(0\) or~\(1\).
\end{exer}

\begin{exer}[9]
If \(f:E\to E\) is a function which commutes with all automorphisms in~\(\GL(E)\), then \(f=h_{\lambda}\) for some \(\lambda\in\R\).
\end{exer}
\begin{proof}
First, since \(f\)~commutes with \(h_2\in\GL(E)\),
\[f(0)=f(2\mult 0)=2\mult f(0)=f(0)+f(0)\]
and it follows that \(f(0)=0\). Now \(f(\alpha x)=\alpha f(x)\) for all \(x\in E\) and \(\alpha\in\R\), using the previous result for \(\alpha=0\) and commutativity of~\(f\) with \(h_{\alpha}\in\GL(E)\) for \(\alpha\ne0\).

Fix \(x\ne 0\) and let \(u\ne 1\) be a transvection in the line \(D=D_{0x}\). Then \(u(f(x))=f(u(x))=f(x)\), so \(f(x)\in D\) and hence \(f(x)=\lambda x\) for some \(\lambda\in\R\). It follows that \(f(y)=\lambda y\) for all \(y\in D\). Fix \(y\not\in D\). We may assume \(u(y)=x+y\). By reasoning as above, \(f(y)=\lambda_y y\) and \(f(x+y)=\lambda_{x+y}(x+y)\) for some \(\lambda_y,\lambda_{x+y}\in\R\). But also
\[f(x+y)=f(u(y))=u(f(y))=u(\lambda_y y)=\lambda_y u(y)=\lambda_y(x+y)\]
so \(\lambda_y=\lambda_{x+y}\) (since \(x+y\ne 0\)). Now considering the transvection~\(u'\) in the line \(D'=D_{0y}\) with \(u'(x)=x+y\), it follows that \(\lambda=\lambda_{x+y}=\lambda_y\). Therefore \(f(y)=\lambda y\) for all \(y\in E\), so \(f=h_{\lambda}\).
\end{proof}

% References
\begin{thebibliography}{0}
\bibitem{dieudonne} Dieudonn\'e, J. \textit{Linear Algebra and Geometry.} Hermann, 1969.
\end{thebibliography}
\end{document}
