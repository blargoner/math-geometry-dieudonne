% Notes and exercises from Linear Algebra and Geometry by Dieudonne
% By John Peloquin
\documentclass[letterpaper,12pt]{article}
\usepackage{amsmath,amssymb,amsthm,enumitem,fourier}

\newcommand{\R}{\mathbf{R}}
\newcommand{\Rnz}{\R^*}
\newcommand{\C}{\mathbf{C}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\U}{\mathbf{U}}

\newcommand{\iso}{\cong}

\newcommand{\union}{\cup}
\newcommand{\sect}{\cap}
\newcommand{\after}{\circ}
\newcommand{\mult}{\cdot}
\newcommand{\cross}{\times}

\let\O\undefined % for want of \RedeclareMathOperator

\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{\mathbf{GL}}
\DeclareMathOperator{\GLp}{\GL^+}
\DeclareMathOperator{\SL}{\mathbf{SL}}
\DeclareMathOperator{\GO}{\mathbf{GO}}
\DeclareMathOperator{\GOp}{\GO^+}
\DeclareMathOperator{\O}{\mathbf{O}}
\DeclareMathOperator{\Op}{\O^+}
\DeclareMathOperator{\SO}{\mathbf{SO}}
\DeclareMathOperator{\GA}{\mathbf{GA}}
\DeclareMathOperator{\Sm}{\mathbf{Sm}}
\DeclareMathOperator{\Is}{\mathbf{Is}}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sector}{S}
\DeclareMathOperator{\osector}{S^{\circ}}

\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\kerz}[1]{\inv{#1}(0)}
\newcommand{\dual}[1]{#1^*}
\newcommand{\abs}[1]{|{#1}|}
\newcommand{\norm}[1]{\lVert{#1}\rVert}
\newcommand{\innerprod}[2]{({#1}\;|\;{#2})}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\adj}[1]{#1^*}
\newcommand{\ang}[2]{\widehat{(#1,#2)}}

% Theorems
\theoremstyle{definition}
\newtheorem*{exer}{Exercise}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

% Meta
\title{Notes and exercises from\\\textit{Linear Algebra and Geometry}}
\author{John Peloquin}
\date{}

\begin{document}
\maketitle

\section*{Introduction}
This document contains notes and exercises from~\cite{dieudonne}.

\subsection*{Groups}
This table summarizes some important groups appearing in the book:\footnote{Legend: V = vector space, E = Euclidean (inner product) space, A = affine, O = oriented}

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Group}&\textbf{Name}&\textbf{Context}&\textbf{Description}\\
\hline
\(\GL(E)\)&General linear group&V&Bijective linear maps\\
\(\GA(E)\)&General affine group&V, A&Bijective affine maps\\
\(\SL(E)\)&Special linear group&V&Linear maps with \(\det=1\)\\
\(\GLp(E)\)&&V, O&Linear maps with \(\det>0\)\\
\(\GO(E)\)&&E&Bijective linear similitudes\\
\(\O(E)\)&Orthogonal group&E&Linear isometries\\
\(\Sm(E)\)&&E, A&Bijective affine similitudes\\
\(\Is(E)\)&Euclidean group&E, A&Affine isometries\\
\(\GOp(E)\)&&E, O&Direct similitudes (\(\det>0\))\\
\(\Op(E)\), \(\SO(E)\)&Rotation group, or&E, O&Rotations (\(\det=1\))\\
&special orthogonal group&&\\
\hline
\end{tabular}
\end{center}

\newpage
\section*{Chapter~III}
\subsection*{Section~1}
\begin{rmk}
If \(V,W\) are supplementary subspaces of~\(E\) and \(c\)~is the unique point in common to the varieties \(a+V\) and \(b+W\) (3.1.15), then \(a+V=c+V\) and \(b+W=c+W\) (3.1.12), so
\[-c+(a+V)=V\qquad\text{and}\qquad -c+(b+W)=W\]
In other words, taking the origin~\(c\) (3.2.21), the varieties become their direction subspaces.
\end{rmk}

\begin{exer}[4]
Let \(V,W\) be a pair of supplementary subspaces of~\(E\). Every subspace~\(U\) containing~\(V\) is the direct sum of~\(V\) with~\(U\sect W\).
\end{exer}
\begin{proof}
If \(u\in U\), then \(u=v+w\) for some \(v\in V\) and \(w\in W\), and \(w=u-v\in U\). So \(U=V+(U\sect W)\), and \(V\sect U\sect W=\{0\}\).
\end{proof}

\subsection*{Section~2}
\begin{exer}[1]
\emph{Projections and idempotents:} If \(p,q\)~are the projections for a direct sum \(E=V+W\), then \(p,q\in\End(E)\) are such that \(p^2=p\), \(q^2=q\), and \(p+q=1\). Conversely, if \(p\in\End(E)\) is such that \(p^2=p\), then \(E=p(E)+\kerz{p}\) is a direct sum. Moreover, if \(q=1-p\), then \(q^2=q\), \(q(E)=\kerz{p}\), and \(\kerz{q}=p(E)\).
\end{exer}
\begin{proof}
For the forward direction, we know \(p,q\in\End(E)\) and \(p+q=1\) (3.2.2). It follows that \(p^2=p\after(1-q)=p-pq=p\) and \(q^2=(1-p)^2=1-p=q\).

For the converse, \(p+q=1\), so \(E=p(E)+q(E)\). Also \(pq=p-p^2=0\), so \(p(E)\sect q(E)=\{0\}\) and \(q(E)\subseteq\kerz{p}\). If \(x\in\kerz{p}\), then \(q(x)=x\), so \(x\in q(E)\). Hence \(q(E)=\kerz{p}\) and similarly \(\kerz{q}=p(E)\). Finally \(q^2=q\) as above.
\end{proof}

\begin{exer}[2]
If \(W\) and~\(W'\) are both supplementary to~\(V\) in~\(E\), then \(W\) and~\(W'\) are isomorphic.
\end{exer}
\begin{proof}
If \(p\)~is the projection of~\(E\) onto~\(W'\), then the restriction of~\(p\) to~\(W\) is an isomorphism from~\(W\) to~\(W'\).
\end{proof}

\begin{exer}[3]
If \(E=V+W\) is a direct sum with inclusions \(i:V\to E\) and \(j:W\to E\), and \(v:V\to F\) and \(w:W\to F\) are linear maps, then there is a unique linear map \(u:E\to F\) with \(u\after i=v\) and \(u\after j=w\).
\end{exer}
\begin{proof}
If \(p,q\) are the projections on \(V,W\) respectively, then \(u=v\after p+w\after q\).
\end{proof}

\begin{exer}[11]
\(\GA(E)/E\iso\GL(E)\).
\end{exer}
\begin{proof}
Define \(\varphi:\GA(E)\to\GL(E)\) by \(\varphi(t_a\after v)=v\). Note that \(\varphi\)~is well-defined by~(3.2.17), \(\varphi\)~is a homomorphism by~(3.2.19), and \(\varphi\)~is clearly surjective. Also \(\varphi(u)=1\) if and only if \(u\)~is a translation, so \(\ker\varphi=T(E)\), the normal subgroup of translations. It follows that \(\GA(E)/T(E)\iso\GL(E)\). Finally, the mapping \(a\mapsto t_a\) is an isomorphism \(E\iso T(E)\) from the additive group~\(E\).
\end{proof}

\begin{exer}[13]
If \(u:E\to F\) is affine and \(L\)~is a variety in~\(F\), then \(\inv{u}(L)\)~is empty or a variety in~\(E\).
\end{exer}
\begin{proof}
If \(a\in\inv{u}(L)\) and \(L_0\)~is the direction of~\(L\), then \(L=u(a)+L_0\) and hence \(\inv{u}(L)=a+\inv{u}(L_0)\).
\end{proof}

\subsection*{Section~3}
\begin{rmk}
If \(a,b\in E\), then the segment~\(ab\) (3.3.4) consists of all points \(x=\alpha a+\beta b\) where \(\alpha,\beta\ge 0\) and \(\alpha+\beta=1\)---the \emph{convex combinations} of \(a\) and~\(b\).
\end{rmk}

\begin{exer}[3]
A necessary and sufficient condition for a nonempty subset~\(V\) of a vector space to be a variety is that for all pairs~\(x,y\) of distinct points of~\(V\), the line~\(D_{xy}\) is contained in~\(V\).
\end{exer}
\begin{proof}
The condition is necessary by~(3.3.2).

If the condition holds, choose \(v\in V\) and let \(V_0=-v+V\). We claim \(V_0\)~is a subspace, from which it follows that \(V=v+V_0\) is a variety. First, \(0=-v+v\in V_0\). If \(x\in V_0\) and \(x\ne 0\), then \(v+x\in V\) and \(v+x\ne v\), so \(D_{v,v+x}=\{\,v+\xi x\mid\xi\in\R\,\}\subseteq V\). It follows that \(\xi x\in V_0\) for all \(\xi\in\R\). If also \(y\in V_0\) and \(y\ne x\), then \(D_{v+x,v+y}\subseteq V\), so in particular \(v+2^{-1}(x+y)\in V\) and \(2^{-1}(x+y)\in V_0\). By the previous result, it then follows that \(x+y\in V_0\). Therefore \(V_0\)~is a subspace as claimed.
\end{proof}

\goodbreak
\begin{exer}[4]
\emph{Translations and homothetic maps:}
\begin{itemize}[itemsep=0pt]
\item A necessary and sufficient condition for an affine map to be a translation or a homothetic map is that its associated linear map be homothetic.
\item A necessary and sufficient condition for an affine map to preserve the direction of lines is that it be a translation or a bijective homothetic map.
\item If \(u_1,u_2\)~are translations or homothetic maps, then so is~\(u_1\after u_2\).
\item If \(u_1,u_2\) and~\(u_1\after u_2\) are homothetic maps with ratios not equal to 1, their centers are collinear. If instead \(u_1\after u_2\)~is a translation, then it is either the identity or a translation in the direction of the line through the centers of \(u_1\) and~\(u_2\).
\item If \(v\in\GA(E)\), then \(v\after h_{a,\lambda}\after\inv{v}=h_{v(a),\lambda}\).
\item The subset~\(H(E)\) of translations and homothetic maps in~\(\GA(E)\) forms a subgroup, and \(H(E)/E\iso\Rnz\).
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item This follows from the equations \(t_a=t_a\after h_1\) and \(h_{a,\lambda}=t_{(1-\lambda)a}\after h_{\lambda}\) and \(t_a\after h_{\lambda}=h_{(1-\lambda)^{-1}a,\lambda}\) (\(\lambda\ne 1\)).

\item The condition is sufficient because such a map has the form \(t_a\after h_{\lambda}\) with \(\lambda\ne 0\), which clearly preserves the direction of lines. Conversely, suppose \(u=t_a\after v\) preserves the direction of lines. If \(x\ne 0\), let \(D\)~be the vector line through~\(x\). Then \(v(D)=D\), so \(v(x)=\lambda x\) for some \(\lambda\in\R\) with \(\lambda\ne0\), and in fact \(v(y)=\lambda y\) for all \(y\in D\). We claim \(v=h_{\lambda}\), from which the result follows. If \(y\not\in D\), then by considering the vector line~\(D'\) through~\(y\) we have \(v(y)=\mu y\) for some \(\mu\in\R\). Now \(v(D_{xy})=D_{v(x)v(y)}=D_{\lambda x,\mu y}\), and since \(v\)~preserves direction there is \(\xi\in\R\) with \(\mu y-\lambda x=\xi(y-x)\), or \((\mu-\xi)y=(\lambda-\xi)x\). Since \(y\not\in D\), this implies \(\mu=\xi=\lambda\). Therefore \(v=h_{\lambda}\) as claimed.

\item If \(u_1=t_a\after h_{\lambda}\) and \(u_2=t_b\after h_{\mu}\), then by~(3.2.19.1), \(u_1\after u_2=t_{a+\lambda b}\after h_{\lambda\mu}\).

\item Write \(u_1=h_{a,\lambda}\), \(u_2=h_{b,\mu}\), and \(u_1\after u_2=h_{c,\nu}\) with \(\lambda,\mu,\nu\ne1\). Then for all~\(x\),
\[(1-\nu)c+\nu x=(1-\lambda)a+\lambda(1-\mu)b+\lambda\mu x\]
Taking \(x=0\) and \(x\ne0\) (we assume such exist!) yields \(\nu=\lambda\mu\) and
\[c=(1-\lambda\mu)^{-1}[(1-\lambda)a+\lambda(1-\mu)b]=(1-\lambda\mu)^{-1}(\lambda-1)(b-a)+b\]
so that \(a,b,c\) are collinear. If instead \(u_1\after u_2=t_c\), then \(\lambda\mu=1\) and \(c=(\lambda-1)(b-a)\), from which the second result follows.

\item Write \(v=t_b\after w\), where \(w\in\GL(E)\), so that \(\inv{v}=\inv{w}\after t_{-b}\). By repeated application of~(3.2.19.1),
\begin{align*}
v\after h_{a,\lambda}\after\inv{v}&=t_b\after w\after t_{(1-\lambda)a}\after h_{\lambda}\after\inv{w}\after t_{-b}\\
	&=t_{v((1-\lambda)a)}\after w\after h_{\lambda}\after\inv{w}\after t_{-b}\\
	&=t_{b+(1-\lambda)w(a)}\after h_{\lambda}\after t_{-b}\\
	&=t_{b+(1-\lambda)w(a)-\lambda b}\after h_{\lambda}\\
	&=t_{(1-\lambda)v(a)}\after h_{\lambda}\\
	&=h_{v(a),\lambda}
\end{align*}

\item First, \(1\in H(E)\). If \(u_1,u_2\in H(E)\), then \(u_1\after u_2\in H(E)\) by a previous item. If \(u_1=t_a\after h_{\lambda}\) with \(\lambda\ne0\), then \(\inv{u_1}=t_{-\lambda^{-1}a}\after h_{\lambda^{-1}}\in H(E)\). Therefore \(H(E)\)~is a subgroup of~\(\GA(E)\). Define \(\varphi:H(E)\to\Rnz\) by \(\varphi(t_a\after h_{\lambda})=\lambda\). Note that \(\varphi\)~is well-defined (since \(E\ne\{0\}\)!), \(\varphi\)~is a homomorphism by a previous item, \(\varphi\)~is surjective, and \(\ker\varphi=T(E)\iso E\). It follows that \(H(E)/E\iso\Rnz\).\qedhere
\end{itemize}
\end{proof}

\begin{exer}[5]
A variety not parallel to a hyperplane meets the hyperplane. If \(H\)~is a vector hyperplane in~\(E\) and \(V\)~is a subspace of~\(E\) not contained in~\(H\), then \(V\sect H\)~is a vector hyperplane in~\(V\).
\end{exer}
\begin{proof}
Any vector in the direction of the variety which is not in the direction of the hyperplane determines a line in the variety which meets the hyperplane (3.3.8).

If \(D\)~is the vector line through a vector in \(V-H\), then \(D\)~is supplementary to~\(H\) in~\(E\), so \(D\)~is supplementary to~\(V\sect H\) in~\(V\) (Exercise~3.1.4).
\end{proof}

\begin{exer}[6]
\emph{Dilations and transvections:} Let \(H\)~be a vector hyperplane in~\(E\) and suppose \(u\in\End(E)\) fixes every element of~\(H\).
\begin{itemize}[itemsep=0pt]
\item There is \(\gamma\in\R\) unique such that \(u(a)\in\gamma a+H\) for all \(a\in E-H\).
\item If \(\gamma\ne 1\), then \(\gamma\)~is an eigenvalue of~\(u\) and \(E(\gamma;u)\)~is a line~\(S\) supplementary to \(E(1;u)=H\). A subspace~\(V\) satisfies \(u(V)\subseteq V\) if and only if \(S\subseteq V\) or \(V\subseteq H\). In particular, a vector line~\(D\) satisfies \(u(D)\subseteq D\) if and only if \(D=S\) or \(D\subseteq H\).
\item If \(\gamma=1\), and \(g(x)=0\) is an equation of~\(H\), there is \(c\in H\) unique such that \(u(x)=x+g(x)c\) for all \(x\in E\). \(u\)~is bijective. If \(u\ne 1\) (so \(c\ne 0\)), then the line \(T=D_{0c}\) is independent of~\(g\). The scalar~\(1\) is the only eigenvalue of~\(u\) if \(H\ne\{0\}\), and \(E(1;u)=H\) if \(u\ne 1\). If \(u\ne 1\), a subspace~\(V\) satisfies \(u(V)\subseteq V\) if and only if \(T\subseteq V\) or \(V\subseteq H\); in particular, a vector line~\(D\) satisfies \(u(D)=D\) if and only if \(D\subseteq H\).
\item The set~\(\Gamma(E,H)\) of automorphisms of~\(E\) leaving the hyperplane~\(H\) fixed pointwise is a subgroup of~\(\GL(E)\). The subset~\(\Theta(E,H)\) of transvections is a normal abelian subgroup of~\(\Gamma(E,H)\) isomorphic to~\(H\). \(\Gamma(E,H)/H\iso\Rnz\).
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item If \(a\in E-H\), then \(E=\R a+H\), so \(u(a)=\gamma a+t\) for some \(\gamma\in\R\) and \(t\in H\). If \(b\in E\), then \(b=\beta a+h\) for some \(\beta\in\R\) and \(h\in H\), so
\begin{align*}
u(b)&=u(\beta a+h)\\
	&=\beta u(a)+u(h)\\
	&=\beta(\gamma a+t)+h\\
	&=\gamma(\beta a+h)+(1-\gamma)h+\beta t\\
	&=\gamma b+(1-\gamma)h+\beta t\tag{1}
\end{align*}
Therefore \(u(b)\in\gamma b+H\). If also \(u(b)\in\gamma'b+H\), then \((\gamma'-\gamma)b\in H\), which implies \(\gamma'=\gamma\) if \(b\not\in H\). Therefore \(\gamma\)~is unique for \(b\not\in H\).

\item Let \(x=a-(1-\gamma)^{-1}t\). Then \(x\ne 0\) since \(a\not\in H\) and
\begin{align*}
u(x)&=u(a-(1-\gamma)^{-1}t)\\
	&=u(a)-(1-\gamma)^{-1}u(t)\\
	&=\gamma a+t-(1-\gamma)^{-1}t\\
	&=\gamma[a-(1-\gamma)^{-1}t]\\
	&=\gamma x
\end{align*}
Therefore \(x\)~is an eigenvector of~\(u\) with eigenvalue~\(\gamma\). Let \(S\)~be the vector line through~\(x\). Then \(S\subseteq E(\gamma;u)\). Conversely if \(b\in E(\gamma;u)\), then \(u(b)=\gamma b\), which implies \((1-\gamma)h+\beta t=0\) in~(1), so \(h=-\beta(1-\gamma)^{-1}t\) and
\[b=\beta a+h=\beta[a-(1-\gamma)^{-1}t]=\beta x\in S\]
Therefore \(S=E(\gamma;u)\). By hypothesis \(H\subseteq E(1;u)\). Conversely if \(b\in E(1;u)\), then \(b=u(b)\in\gamma b+H\), so \((1-\gamma)b\in H\), so \(b\in H\). Therefore \(H=E(1;u)\). \(S\)~is supplementary to~\(H\) since \(x\not\in H\).

By hypothesis \(u(V)\subseteq V\) for any subspace \(V\subseteq H\). If \(S\subseteq V\), then \(V=S+V\sect H\) (Exercise~3.1.4), so clearly \(u(V)\subseteq V\). Conversely if \(u(V)\subseteq V\) and \(v\in V-H\), then \(v=s+h\) for some \(s\in S\) with \(s\ne 0\) and \(h\in H\), so \(u(v)=\gamma s+h\) and \(v-u(v)=(1-\gamma)s\in V\), which implies \(s\in V\) and \(S=\R s\subseteq V\).

\item Fix \(e\in E\) with \(g(e)=1\) and let \(c=u(e)-e\). Since \(u(e)\in e+H\), \(g(c)=0\) and \(c\in H\). Now \(u(x)=x+g(x)c\) holds for \(x=e\), and for \(x\in H\), so by linearity it holds for all \(x\in \R e+H=E\). Note \(c\)~is unique since if \(u(e)=e+g(e)c'\), then \(c'=u(e)-e=c\).

The map \(x\mapsto x-g(x)c\) is clearly the inverse of~\(u\), so \(u\)~is bijective.

If \(h(x)=0\) is another equation of~\(H\), then by the above there exists \(c'\in H\) such that \(u(x)=x+h(x)c'\) for all \(x\in E\). But \(h=\lambda g\) for some \(\lambda\ne 0\) (3.3.6), so \(u(x)=x+g(x)(\lambda c')\) for all \(x\in E\) and \(c=\lambda c'\) by uniqueness of~\(c\). If \(u\ne 1\), then \(T=D_{0c}=D_{0c'}\) is independent of~\(g\).

If \(u(x)=x+g(x)c=\lambda x\), then \((\lambda-1)x\in H\). If \(\lambda\ne 1\), then \(x\in H\), so actually \((\lambda-1)x=0\) and \(x=0\). If \(\lambda=1\), then \(g(x)c=0\), so either \(g(x)=0\) and \(x\in H\), or \(c=0\) and \(u=1\).

As above, \(u(V)\subseteq V\) if \(V\subseteq H\) or \(T\subseteq V\). Conversely if \(u(V)\subseteq V\) and \(x\in V-H\), then \(g(x)\ne 0\) and \(g(x)c=u(x)-x\in V\), so \(c\in V\) and \(T=\R c\subseteq V\).\qedhere

\item \(\Gamma(E,H)\)~is obviously a subgroup of~\(\GL(E)\). Define \(\varphi:\Gamma(E,H)\to\Rnz\) by \(\varphi(u)=\gamma\). Then \(\varphi\)~is a well-defined, surjective homomorphism and \(\ker\varphi=\Theta(E,H)\). It follows that \(\Theta(E,H)\)~is a normal subgroup of~\(\Gamma(E,H)\) and that \(\Gamma(E,H)/\Theta(E,H)\iso\Rnz\). Finally, the mapping \(c\mapsto(x\mapsto x+g(x)c)\) is an isomorphism \(H\iso\Theta(E,H)\), so in particular \(\Theta(E,H)\)~is abelian.
\end{itemize}
\end{proof}

\section*{Chapter~IV}
\subsection*{Section~1}
\begin{exer}[1]
If \(t\ne1\) is a transvection in~\(E\), a basis for~\(E\) may be chosen relative to which
\[M(t)=\begin{pmatrix}
1&1\\
0&1
\end{pmatrix}\]
Conversely, matrices of the form
\[B_{12}(\lambda)=\begin{pmatrix}
1&\lambda\\
0&1
\end{pmatrix}
\qquad
B_{21}(\lambda)=\begin{pmatrix}
1&0\\
\lambda&1
\end{pmatrix}\]
are matrices of transvactions relative to some basis~\(\{a_1,a_2\}\). These transvections have the lines \(D_{0a_1}\) and~\(D_{0a_2}\) respectively.
\end{exer}
\begin{proof}
Write \(t(x)=x+g(x)c\), where \(g\in\dual{E}\) with \(g\ne 0\), and \(c\ne 0\) with \(g(c)=0\) (Exercise~3.3.6). Let \(a_1=c\) and choose~\(a_2\) such that \(g(a_2)=1\). Then \(\{a_1,a_2\}\)~is the desired basis for~\(E\).

Conversely, if \(t\)~is the transformation of~\(B_{12}(\lambda)\) relative to~\(\{a_1,a_2\}\), then
\begin{align*}
t(x)&=t(\xi_1a_1+\xi_2a_2)\\
	&=\xi_1 t(a_1)+\xi_2 t(a_2)\\
	&=\xi_1 a_1+\xi_2(\lambda a_1+a_2)\\
	&=\xi_1 a_1+\xi_2 a_2+\lambda\xi_2 a_1\\
	&=x+g(x)a_1
\end{align*}
where \(g=\lambda\dual{a_2}\in\dual{E}\). Therefore \(t\)~is a transvection in the line~\(D_{0a_1}\). A similar argument applies to~\(B_{21}(\lambda)\).
\end{proof}

\begin{exer}[3]
If \(u\in\End(E)\) and \(\rank(u)=1\), there is a basis of~\(E\) relative to which
\[M(u)=\begin{pmatrix}
\delta&0\\
0&0
\end{pmatrix}
\quad(\delta\ne 0)
\qquad\text{or}\qquad
M(u)=\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}\]
The second case occurs if and only if \(u\)~is nilpotent, in which case \(u^2=0\).
\end{exer}
\begin{proof}
Let \(N=\kerz{u}\) and \(R=u(E)\). Then \(N\) and~\(R\) are vector lines in~\(E\) (4.1.7). If \(N\sect R=\{0\}\), then \(E=N+R\) is a direct sum. Choose \(a_1\in R\) and \(a_2\in N\) with \(a_1,a_2\ne 0\). Then \(u(a_1)=\delta a_1\) with \(\delta\ne 0\) since \(u(a_1)\in R\) and \(a_1\not\in N\), and also \(u(a_2)=0\). It follows that \(\{a_1,a_2\}\)~is a basis of~\(E\) relative to which
\[M(u)=\begin{pmatrix}
\delta&0\\
0&0
\end{pmatrix}\]
If \(N\sect R\ne\{0\}\), then \(N=R\) (3.3.1). Choose \(a_1\in N\), \(a_1\ne 0\) and \(a_2\)~with \(u(a_2)=a_1\). Then \(\{a_1,a_2\}\)~is a basis of~\(E\) (since \(a_2\not\in N\)) relative to which
\[M(u)=\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}\]
If \(M(u)\)~has this form, then \(M(u^2)=M(u)^2=0\), so \(u^2=0\) and \(u\)~is nilpotent. Conversely if \(u\)~is nilpotent, there is \(k>1\) least such that \(u^k=0\). Then \(u^{k-1}(E)\ne 0\) but \(u^{k-1}(E)\subseteq N\), so \(u^{k-1}(E)=N\). If \(k>2\), then \(u^{k-1}(E)\)~is a proper subspace of~\(u^{k-2}(E)\), lest \(u^{k-2}(E)=N\) and \(u^{k-1}=0\). But then \(u^{k-2}(E)=E\), impossible since \(\rank(u)=1\). It follows that \(u^2=0\), so \(N=R\) and \(M(u)\)~has this form.
\end{proof}

\begin{exer}[7]
Let \(u:E\to E\) be an injective function such that \(E\)~is the smallest variety containing~\(u(E)\), and \(u(a),u(b),u(c)\) are collinear whenever \(a,b,c\in E\) are collinear.
\begin{itemize}[itemsep=0pt]
\item If \(a,b,c\in E\) are not collinear, then \(u(a),u(b),u(c)\) are not collinear.
\item For every line~\(D\), there is a unique line~\(D'\) such that \(u(D)\subseteq D'\). If \(u\)~is bijective, then \(u(D)=D'\); moreover, if \(D_1,D_2\) are parallel (resp. distinct, not parallel), then so are \(u(D_1),u(D_2)\).
\item If \(u\)~is bijective, there is \(v\in\GA(E)\) such that \(u_1=v\after u\) fixes the origin and basis vectors \(a_1,a_2\in E\). \(u_1\)~maps lines to lines and preserves parallelism (resp. distinctness, non-parallelism); in particular, \(u_1\)~preserves the lines \(D_{0a_1},D_{0a_2},D_{a_1a_2}\) and hence the directions of any lines parallel to these.
\item Given the points \(\xi a_1,\eta a_1\), it is possible to construct \((\xi+\eta)a_1\) and~\(\xi\eta a_1\) by intersecting lines with direction vectors derived from~\(a_1,a_2\).
\item If \(\varphi\)~is defined by \(u_1(\xi a_1)=\varphi(\xi) a_1\), then \(\varphi\)~is a field automorphism of~\(\R\), so \(\varphi=1_{\R}\).\footnote{For this problem, we assume there are no nontrivial field automorphisms of~\(\R\).} It follows that \(u_1=1_E\), so \(u=\inv{v}\) is an affine map.
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item Suppose towards a contradiction that \(u(a),u(b),u(c)\) are on the line~\(\Delta\). Let \(x\in E\). If \(x\in D_{ab}\union D_{ac}\union D_{bc}\), then \(u(x)\in\Delta\). Otherwise if, say, \(x\) and~\(c\) are on opposite sides of~\(D_{ab}\), then \(D_{xc}\) and~\(D_{ab}\) intersect at a unique point~\(y\) by (3.3.9) and~(4.1.6), \(u(y)\in\Delta\) since \(y\in D_{ab}\), and \(u(x)\in\Delta\) since \(x\in D_{yc}\). Similarly \(u(x)\in\Delta\) if \(x\) and~\(b\) are on opposite sides of~\(D_{ac}\), or \(x\) and~\(a\) are on opposite sides of~\(D_{bc}\). Finally, if none of these cases hold, then \(D_{xc}\)~cannot be parallel to~\(D_{ab}\), because if \(x\)~is in the direction of~\(a-b\) from~\(c\) (that is, if \(x=c+\xi(a-b)\) for \(\xi>0\)) then \(x\) and~\(b\) are on opposite sides of~\(D_{ac}\), and if \(x\)~is in the direction of~\(b-a\) from~\(c\) then \(x\) and~\(a\) are on opposite sides of~\(D_{bc}\). Therefore \(D_{xc}\) and~\(D_{ab}\) intersect at a unique point~\(y\) by~(4.1.6) and \(u(x)\in\Delta\) as above. Since \(x\)~was arbitrary, this means \(u(E)\subseteq\Delta\), contradicting the hypothesis about~\(u(E)\).

\item By hypothesis, \(u(D_{ab})\subseteq D_{u(a)u(b)}\). If \(u\)~is bijective and \(x\in D_{u(a)u(b)}\), then \(x=u(c)\) for some \(c\in D_{ab}\) by the previous item, so \(D_{u(a)u(b)}\subseteq u(D_{ab})\). If \(D_1,D_2\) are distinct and parallel, then \(D_1\sect D_2=\emptyset\), so \(u(D_1)\sect u(D_2)=\emptyset\) by injectivity of~\(u\), so \(u(D_1),u(D_2)\) are distinct and parallel. If \(D_1,D_2\) are not parallel, they intersect at a unique point~\(a\). If \(b\in D_1\) and \(c\in D_2\) with \(b,c\ne a\), then \(a,b,c\) are not collinear, so \(u(a),u(b),u(c)\) are not collinear by the previous item and \(u(D_1),u(D_2)\) are not parallel.

\item Let \(a_1,a_2\) be basis vectors of~\(E\). Then \(0,a_1,a_2\) are not collinear (4.1.1), so \(u(0),u(a_1),u(a_2)\) are not collinear by a previous item, so \(a_1'=u(a_1)-u(0)\) and \(a_2'=u(a_2)-u(0)\) are basis vectors of~\(E\). Let \(w\in\GL(E)\) map \(a_1'\mapsto a_1\) and \(a_2'\mapsto a_2\) (4.1.10) and let \(v=w\after t_{-u(0)}\in\GA(E)\) (3.2.19). Then \(u_1=v\after u\) fixes \(0,a_1,a_2\). \(u_1\)~operates as claimed on lines by (3.2.17) and the previous item, and the observation that, for example,
\[u_1(D_{a_1a_2})=D_{u_1(a_1)u_1(a_2)}=D_{a_1a_2}\]

\item If \(L_1\)~is the line through~\(\eta a_1\) with direction vector~\(a_2-a_1\), then \(L_1\sect D_{0a_2}=\{\eta a_2\}\), so \(\eta a_2\)~is constructible. If \(L_2\)~is the line through~\(\xi a_1\) with direction vector~\(a_2\), and \(L_3\)~is the line through~\(\eta a_2\) with direction vector~\(a_1\), then \(L_2\sect L_3=\{\xi a_1+\eta a_2\}\), so \(\xi a_1+\eta a_2\) is constructible. If \(L_4\)~is the line through \(\xi a_1+\eta a_2\) with direction vector~\(a_2-a_1\), then \(L_4\sect D_{0a_1}=\{(\xi+\eta)a_1\}\), so \((\xi+\eta)a_1\)~is constructible. If \(L_5\)~is the line through~\(\eta a_2\) with direction vector \(a_2-\xi a_1\), then \(L_5\sect D_{0a_1}=\{\xi\eta a_1\}\), so \(\xi\eta a_1\)~is constructible.

\item By previous items, \(u_1(L_2)\)~is the line passing through~\(u_1(\xi a_1)\) parallel to~\(L_2\) (and to~\(D_{0a_2}\)), and \(u_1(L_3)\)~is the line passing through~\(u_1(\eta a_2)\) parallel to~\(L_3\) (and to~\(D_{0a_1}\)). Since \(u_1(\eta a_2)\in D_{0a_2}\) and \(u_1(\xi a_1)\in D_{0a_1}\), it follows that \(u_1(\xi a_1)+u_1(\eta a_2)\) is the intersection of \(u_1(L_2)\) and~\(u_1(L_3)\). But we also have \(u_1(\xi a_1+\eta a_2)\in u_1(L_2\sect L_3)=u_1(L_2)\sect u_1(L_3)\), so
\[u_1(\xi a_1+\eta a_2)=u_1(\xi a_1)+u_1(\eta a_2)\]
Now \(u_1(L_4)\)~is the line passing through this point and~\(u_1((\xi+\eta)a_1)\) parallel to the line~\(u_1(L_1)\), which in turn passes through \(u_1(\eta a_1)\) and~\(u_1(\eta a_2)\) parallel to~\(L_1\) (and to~\(D_{a_1a_2}\)). Therefore
\[u_1((\xi+\eta)a_1)=u_1(\xi a_1)+u_1(\eta a_1)\]
is the intersection of~\(u_1(L_4)\) and~\(D_{0a_1}\). It follows that \(\varphi(\xi+\eta)=\varphi(\xi)+\varphi(\eta)\).

We claim \(u_1(\eta a_2)=\varphi(\eta)a_2\). Indeed, \(u_1(\eta a_2)=\lambda a_2\) for some~\(\lambda\), and also \(u_1(\eta a_2)=u_1(\eta a_1)+\mu(a_2-a_1)=\varphi(\eta)a_1+\mu(a_2-a_1)\) for some~\(\mu\) by facts about~\(u_1(L_1)\) above. It follows that \(\lambda=\mu=\varphi(\eta)\) by linear independence of~\(a_1,a_2\), establishing the claim.

Now \(u_1(L_5)\)~is the line passing through~\(u_1(\xi\eta a_1)\) and~\(u_1(\eta a_2)\), and with the direction vector \(u_1(\xi a_1)-a_2\). Therefore
\[u_1(\xi\eta a_1)=u_1(\eta a_2)+\varphi(\eta)[u_1(\xi a_1)-a_2]=\varphi(\eta)u_1(\xi a_1)\]
is the intersection of~\(u_1(L_5)\) and~\(D_{0a_1}\). It follows that \(\varphi(\xi\eta)=\varphi(\xi)\varphi(\eta)\). Since \(\varphi(1)=1\), \(\varphi\)~is a field automorphism of~\(\R\) and hence \(\varphi=1_{\R}\).

By the above, \(u_1\)~fixes \(D_{0a_1}\) and~\(D_{0a_2}\) pointwise. If \(x=\xi_1 a_1+\xi_2 a_2\), then \(x\)~is the intersection of the line through~\(\xi_1 a_1\) parallel to~\(D_{0a_2}\) and the line through~\(\xi_2 a_2\) parallel to~\(D_{0a_1}\). It follows that \(u_1(x)\)~is the intersection of the same lines, so \(u_1(x)=x\). Therefore \(u_1=1_E\), and \(u=\inv{v}\) is affine.\qedhere
\end{itemize}
\end{proof}

\begin{exer}[8]
We have
\[\begin{pmatrix}
\alpha_1\\
\alpha_2
\end{pmatrix}
\begin{pmatrix}
\beta_1&\beta_2
\end{pmatrix}
=
\begin{pmatrix}
\alpha_1\beta_1&\alpha_1\beta_2\\
\alpha_2\beta_1&\alpha_2\beta_2
\end{pmatrix}\]
This can be interpreted as the composite of a linear form \(E\to\R\) and a linear map \(\R\to E\); it maps the plane onto the origin or onto a vector line and hence is of rank \(0\) or~\(1\).
\end{exer}

\begin{exer}[9]
If \(f:E\to E\) is a function which commutes with all automorphisms in~\(\GL(E)\), then \(f=h_{\lambda}\) for some \(\lambda\in\R\).
\end{exer}
\begin{proof}
First, since \(f\)~commutes with \(h_2\in\GL(E)\),
\[f(0)=f(2\mult 0)=2\mult f(0)=f(0)+f(0)\]
and it follows that \(f(0)=0\). Now \(f(\alpha x)=\alpha f(x)\) for all \(x\in E\) and \(\alpha\in\R\), using the previous result for \(\alpha=0\) and commutativity of~\(f\) with \(h_{\alpha}\in\GL(E)\) for \(\alpha\ne0\).

Fix \(x\ne 0\) and let \(u\ne 1\) be a transvection in the line \(D=D_{0x}\). Then \(u(f(x))=f(u(x))=f(x)\), so \(f(x)\in D\) and hence \(f(x)=\lambda x\) for some \(\lambda\in\R\). It follows that \(f(y)=\lambda y\) for all \(y\in D\). Fix \(y\not\in D\). We may assume \(u(y)=x+y\). By reasoning as above, \(f(y)=\lambda_y y\) and \(f(x+y)=\lambda_{x+y}(x+y)\) for some \(\lambda_y,\lambda_{x+y}\in\R\). But also
\[f(x+y)=f(u(y))=u(f(y))=u(\lambda_y y)=\lambda_y u(y)=\lambda_y(x+y)\]
so \(\lambda_y=\lambda_{x+y}\) (since \(x+y\ne 0\)). Now considering the transvection~\(u'\) in the line \(D'=D_{0y}\) with \(u'(x)=x+y\), it follows that \(\lambda=\lambda_{x+y}=\lambda_y\). Therefore \(f(y)=\lambda y\) for all \(y\in E\), so \(f=h_{\lambda}\).
\end{proof}

\subsection*{Section~2}
\begin{rmk}
In~(4.2.5), if \(a\in E\), \(a\ne 0\) and \(f(x)=0\) is an equation of~\(D_{0a}\) with \(f\in\dual{E}\), \(f\ne 0\) (3.3.6), choose \(b\in E\) with \(f(b)=1\). Then \(\{a,b\}\)~is a basis of~\(E\). Let \(\Psi\)~be the alternating bilinear form on~\(E\) with \(\Psi(a,b)=1\). Then for all \(x=\xi a+\eta b\),
\[\Psi(a,x)=\eta=f(x)\]
Since \(\Psi\)~is essentially the determinant (4.2.9) and the determinant measures (oriented) area, this just says that \(x\)~is on the vector line determined by~\(a\) if and only if the parallelogram determined by \(x\) and~\(a\) has zero area.
\end{rmk}

\begin{rmk}
An alternating bilinear form \(\Psi\ne 0\) on~\(E\) captures linear independence in that \(\Psi(x,y)\ne 0\) if and only if \(\{x,y\}\)~is independent (4.2.5). It follows that if \(u\in\End(E)\), then \(\det(u)\ne 0\) if and only if \(u\)~preserves linear independence (4.2.6.1), which is true if and only if \(u\)~is bijective (4.1.8). This is just~(4.2.8).
\end{rmk}

\begin{exer}[3]
Relative to a fixed basis of~\(E\), let \(u\) and~\(v\) be defined by
\[M(u)=\begin{pmatrix}
1&0\\
0&0
\end{pmatrix}
\qquad
M(v)=\begin{pmatrix}
0&1\\
0&0
\end{pmatrix}\]
Then \(uv=v\), so \(\rank(uv)=1\), and \(vu=0\), so \(\rank(vu)=0\), but \(\lambda=0\) is the only eigenvalue of \(uv\) and~\(vu\), so \(\lambda^2=0\)~is the characteristic equation of \(uv\) and~\(vu\).
\end{exer}

\begin{exer}[5]
If \(u\in\End(E)\) has eigenvalues, there is a basis of~\(E\) relative to which
\[M(u)=\begin{pmatrix}
\lambda&0\\
0&\mu
\end{pmatrix}
\qquad\text{or}\qquad
M(u)=\begin{pmatrix}
\lambda&1\\
0&\lambda
\end{pmatrix}\]
If, relative to some basis of~\(E\),
\[M(u)=\begin{pmatrix}
\lambda&\alpha\\
0&\lambda
\end{pmatrix}\]
with \(\alpha\ne 0\), then
\[M(u)=\begin{pmatrix}
\lambda&1\\
0&\lambda
\end{pmatrix}\]
relative to some basis of~\(E\), but there is no basis of~\(E\) relative to which
\[M(u)=\begin{pmatrix}
\lambda&0\\
0&\lambda
\end{pmatrix}\]
\end{exer}
\begin{proof}
If \(u\)~has two distinct eigenvalues~\(\lambda,\mu\) with eigenvectors~\(a,b\) respectively, then \(\{a,b\}\)~is a basis of~\(E\) and
\[M(u,\{a,b\})=\begin{pmatrix}
\lambda&0\\
0&\mu
\end{pmatrix}\]
Otherwise, \(u\)~has just one eigenvalue~\(\lambda\). If there are two linearly independent eigenvectors~\(a,b\), then
\[M(u,\{a,b\})=\begin{pmatrix}
\lambda&0\\
0&\lambda
\end{pmatrix}\]
Note \(M(u)\)~has this form only if \(u=h_{\lambda}\). If there are not two linearly independent eigenvectors, let \(a\)~be an eigenvector and \(\{a,x\}\)~a basis of~\(E\). Then \(u(x)=\alpha a+\lambda x\) with \(\alpha\ne 0\) by assumption and~(4.2.14), so
\[M(u,\{a,x\})=\begin{pmatrix}
\lambda&\alpha\\
0&\lambda
\end{pmatrix}\]
If \(b=\alpha^{-1}x\), then \(\{a,b\}\)~is a basis of~\(E\) and
\[M(u,\{a,b\})=\begin{pmatrix}
\lambda&1\\
0&\lambda
\end{pmatrix}\]
Note \(M(u)\)~has this form only if \(u\ne h_{\lambda}\).
\end{proof}

\section*{Chapter~V}
\subsection*{Section~1}
\begin{rmk}
For all \(x,y\in E\),
\[\norm{x+y}^2+\norm{x-y}^2=2\left(\norm{x}^2+\norm{y}^2\right)\]
\end{rmk}
\begin{proof}
Bilinearity of the inner product.
\end{proof}
\noindent This is called the \emph{parallelogram law} because it shows that the sum of the squares of the lengths of the diagonals of a parallelogram is equal to the sum of the squares of the lengths of the sides.

\begin{rmk}
If \(a,b\in E\) with \(a\ne b\) and \(x=\alpha a+\beta b\) with \(\alpha,\beta\ge 0\) and \(\alpha+\beta=1\) is a point on the segment~\(ab\) (see remark from~(3.3) above), then
\[\alpha=\frac{\norm{x-b}}{\norm{a-b}}\qquad\beta=\frac{\norm{x-a}}{\norm{a-b}}\]
In other words, \(\alpha\)~is the ratio of the lengths of the segments \(xb\) and~\(ab\), and \(\beta\)~that of \(xa\) and~\(ab\).
\end{rmk}
\begin{proof}
For example, \(x-b=\alpha a+(\beta-1)b=\alpha(a-b)\), so \(\norm{x-b}=\alpha\norm{a-b}\).
\end{proof}

\begin{rmk}
In~(5.1.8), \(H(x)=x+H_0\) and \(D(x)=x+D_0\). To see that the unique point \(y\in D\sect H(x)\) (3.3.8) satisfies the properties for~\(x\), let \(c\in D\sect H\) and observe that \(y-c\) is the orthogonal projection of \(x-c\) on~\(D_0\). Indeed, \(y-c\in D_0\) since \(y,c\in D\) and \((x-c)-(y-c)=x-y\in H_0\) since \(y\in H(x)\). It follows (5.1.7) that \(x-y\)~is orthogonal to \(y'-y\in D_0\) for all \(y'\in D\) and
\[d(x,y)=d(x-c,y-c)\le d(x-c,d)=d(x,d+c)\]
for all \(d\in D_0\), so \(d(x,y)\le d(x,y')\) for all \(y'\in D\) (since \(c+D_0=D\)). Moreover, \(y\)~is unique in satisfying these properties for~\(x\) since \(y-c\)~is unique in satisfying them for~\(x-c\) (5.1.7). A similar argument applies for the point \(z\in H\sect D(x)\).
\end{rmk}

\begin{rmk}
In~(5.1.15), we initially suspect that \(u\)~is a translation after a symmetry (5.1.13), and hence a symmetry itself. Since \(u(u(0))=u^2(0)=0\), we suspect that \(u\)~is a symmetry about a variety through \(a=\tfrac{1}{2}u(0)\) perpendicular to~\(a\), which turns out to be the case.

By~(5.1.13),
\[\tfrac{1}{2}(x+u(x))=a+\tfrac{1}{2}(x+v(x))\in a+V\]
and
\[\tfrac{1}{2}(x+u(x))-x=a-\tfrac{1}{2}(x-v(x))\in a+W=W\]
since \(-v\)~is the symmetry about~\(W\) and \(a\in W\). Therefore \(\tfrac{1}{2}(x+u(x))\) is the unique point in the intersection of \(a+V\) and \(x+W\) (3.3.8).
\end{rmk}

\begin{rmk}
In~(5.1.16), \(t_{2b}=(t_bu_1)(u_1t_b)\) is the composite of two symmetries about varieties parallel to~\(V\) differing by~\(b\). Therefore \emph{translations are just composites of symmetries.}
\end{rmk}

\begin{exer}[1]
If \(D\)~is a vector line in~\(E\) and \(H\)~is the orthogonal hyperplane, then \(D\)~is the intersection of the vector hyperplanes orthogonal to vector lines in~\(H\) (that is, the vector hyperplanes \emph{perpendicular} to~\(H\)). Dually, \(H\)~is the union of the vector lines orthogonal to~\(D\).
\end{exer}
\begin{proof}
If \(H'\)~is a vector hyperplane orthogonal to a vector line \(D'\subseteq H\), then \(D\)~is orthogonal to~\(D'\) (since \(D\)~is orthogonal to~\(H\)), so \(D\subseteq H'\) (since \(H'\)~is the orthogonal subspace of~\(D'\)). Conversely, if \(x\)~is in the intersection and \(h\in H\), \(h\ne 0\), then \(x\)~is orthogonal to~\(h\) since \(x\)~is in the vector hyperplane orthogonal to the vector line~\(D_{0h}\). The dual follows by taking orthogonal subspaces.
\end{proof}

\begin{exer}[2]
If \(a\) and~\(a'\) are diametrically opposed points on a sphere~\(S\), then a necessary and sufficient condition for a point~\(x\) to be on~\(S\) is that \(x-a\) and \(x-a'\) be orthogonal.
\end{exer}
\begin{proof}
Without loss of generality, we may assume that \(S\)~is centered at the origin with radius~\(\rho\). Then \(a'=-a\), so
\[\innerprod{x-a}{x-a'}=\norm{x}^2-\rho^2=0\iff\norm{x}=\rho\iff x\in S\qedhere\]
\end{proof}

\begin{exer}[4]
\emph{Powers and coorthogonal spheres:}
\begin{itemize}[itemsep=0pt]
\item Let \(S\)~be a sphere centered at~\(c\) with radius~\(\rho\). If \(a\in E\) with \(\delta=d(a,c)\) and \(D\)~is a line through~\(a\) meeting~\(S\) at distinct points \(x_1,x_2\), then
\[\innerprod{x_1-a}{x_2-a}=\delta^2-\rho^2\]
If instead \(D\)~meets \(S\) at the single point~\(x\), then
\[d(a,x)^2=\delta^2-\rho^2\]
\item If \(S_1,S_2\) are non-concentric spheres with respective centers \(c_1,c_2\) and radii \(\rho_1,\rho_2\), then the set~\(H\) of points whose powers with respect to \(S_1\) and~\(S_2\) are equal is a hyperplane perpendicular to~\(D_{c_1c_2}\) and containing \(S_1\sect S_2\).
\item Let \(\pi_k(x)=d(x,c_k)^2-\rho_k^2\)~denote the power of~\(x\) with respect to~\(S_k\) (for \(k=1,2\)). Then the following are equivalent:
\begin{itemize}[itemsep=0pt]
\item[(a)] \(\pi_2(c_1)=\rho_1^2\)
\item[(b)] \(\pi_1(c_2)=\rho_2^2\)
\item[(c)] \(\emptyset\ne S_1\sect S_2\subseteq\{\,x\mid\innerprod{x-c_1}{x-c_2}=0\,\}\)
\item[(d)] \(\pi_1(x)+\pi_2(x)=2\innerprod{x-c_1}{x-c_2}\)
\end{itemize}
\end{itemize}
\end{exer}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item Without loss of generality, we may assume that \(c=0\). In the first case, let \(b=\tfrac{1}{2}(x_1+x_2)\) and \(y=\tfrac{1}{2}(x_1-x_2)\). Then
\[\innerprod{b}{y}=\tfrac{1}{4}(\norm{x_1}^2-\norm{x_2}^2)=\tfrac{1}{4}(\rho^2-\rho^2)=0\]
Since \(y\)~is a direction vector of~\(D\), it follows that \(b\)~is perpendicular to~\(D\). Now \(x_1=b+y\) and \(x_2=b-y\), so \(x_1-a=(b-a)+y\) and \(x_2-a=(b-a)-y\), hence
\begin{align*}
\innerprod{x_1-a}{x_2-a}&=\innerprod{(b-a)+y}{(b-a)-y}&&\\
	&=\innerprod{b-a}{b-a}-\norm{y}^2&&\\
	&=\delta^2-(\norm{b}^2+\norm{y}^2)&&\text{as }\innerprod{b}{b-a}=0\text{, }\innerprod{b}{a}=\norm{b}^2\\
	&=\delta^2-\rho^2&&\text{by Pythagoras (5.1.5)}
\end{align*}
In the second case, \(D\)~must be perpendicular to~\(x\). Indeed, if not, let \(d\)~be a direction vector of~\(D\) with \(\innerprod{d}{x}\ne 0\) and consider
\[z=x-\frac{2\innerprod{d}{x}}{\norm{d}^2}d\]
Then \(z\in D\), \(z\ne x\), and expansion of \(\innerprod{z}{z}\) shows that \(\norm{z}^2=\norm{x}^2\), so \(\norm{z}=\rho\) and \(z\in S\), contradicting that \(D\)~is tangential to~\(S\) at~\(x\). It follows that \(\innerprod{x}{a-x}=0\), and \(d(a,x)^2=\delta^2-\rho^2\) by Pythagoras (5.1.5).

\item We first find \(c\in H\sect D_{c_1c_2}\). We must have \(c=\alpha c_1+\beta c_2\) with \(\alpha,\beta\ge 0\) and \(\alpha+\beta=1\). In fact, \(\alpha=\norm{c-c_2}/\delta\) and \(\beta=\norm{c-c_1}/\delta\) where \(\delta=\norm{c_1-c_2}\ne 0\), so that
\begin{align*}
\alpha-\beta&=(\alpha-\beta)(\alpha+\beta)\\
	&=\alpha^2-\beta^2\\
	&=\frac{\norm{c-c_2}^2-\norm{c-c_1}^2}{\delta^2}\\
	&=\frac{(\norm{c-c_2}^2-\rho_2^2)+\rho_2^2-(\norm{c-c_1}^2-\rho_1^2)-\rho_1^2}{\delta^2}\\
	&=\frac{\rho_2^2-\rho_1^2}{\delta^2}
\end{align*}
since \(c\in H\). It follows that
\[\alpha=\frac{1}{2}+\frac{\rho_2^2-\rho_1^2}{2\delta^2}\qquad\beta=\frac{1}{2}-\frac{\rho_2^2-\rho_1^2}{2\delta^2}\qquad c=\frac{1}{2}(c_1+c_2)+\frac{\rho_2^2-\rho_1^2}{2\delta^2}(c_1-c_2)\]
Note \(c\)~is the midpoint of \(c_1\) and~\(c_2\) if and only if \(\rho_1=\rho_2\). Now
\begin{align*}
x\in H&\iff\norm{x-c_1}^2-\rho_1^2=\norm{x-c_2}^2-\rho_2^2\\
	&\iff\innerprod{x-c_2}{x-c_2}-\innerprod{x-c_1}{x-c_1}=\rho_2^2-\rho_1^2\\
	&\iff\innerprod{x}{c_1-c_2}=\tfrac{1}{2}(\norm{c_1}^2-\norm{c_2}^2+\rho_2^2-\rho_1^2)=\innerprod{c}{c_1-c_2}
\end{align*}
Therefore \(H\)~is the hyperplane through~\(c\) perpendicular to~\(D_{c_1c_2}\). Finally, \(S_1\sect S_2\subseteq H\) since \(S_1\sect S_2\) consists of the points whose powers with respect to \(S_1\) and~\(S_2\) are zero.

\item Let \(\delta=\norm{c_1-c_2}\). We have
\[\pi_2(c_1)=\delta^2-\rho_2^2=\rho_1^2\iff\rho_2^2=\delta^2-\rho_1^2=\pi_1(c_2)\]
so (a)\(\iff\)(b), and these are equivalent to \(\delta^2=\rho_1^2+\rho_2^2\). If this condition holds, then \(\rho_1-\delta\le 0\), so \(2\rho_1(\rho_1-\delta)\le 0\) and
\begin{align*}
2\rho_1^2-2\rho_1\delta+\rho_2^2&\le\rho_2^2\le2\rho_1^2+2\rho_1\delta+\rho_2^2\\
\rho_1^2-2\rho_1\delta+\delta^2&\le\rho_2^2\le\rho_1^2+2\rho_1\delta+\delta^2\\
(\rho_1-\delta)^2&\le\rho_2^2\le(\rho_1+\delta)^2\\
\abs{\rho_1-\delta}&\le\rho_2\le\rho_1+\delta
\end{align*}
so \(S_1\sect S_2\ne\emptyset\) (5.1.11.4). If \(x\in S_1\sect S_2\), then
\[\norm{x-c_1}^2+\norm{x-c_2}^2=\rho_1^2+\rho_2^2=\delta^2=\norm{c_1-c_2}^2\]
so \(\innerprod{x-c_1}{x-c_2}=0\) (5.1.1.4). Therefore (a),(b)\(\implies\)(c). Conversely, if \(x\in S_1\sect S_2\) and \(\innerprod{x-c_1}{x-c_2}=0\), then by Pythagoras (5.1.5.1),
\[\delta^2=\norm{c_1-c_2}^2=\norm{x-c_1}^2+\norm{x-c_2}^2=\rho_1^2+\rho_2^2\]
so (c)\(\implies\)(a),(b). Finally, by~(5.1.1.4),
\begin{align*}
\pi_1(x)+\pi_2(x)&=\norm{x-c_1}^2+\norm{x-c_2}^2-(\rho_1^2+\rho_2^2)\\
	&=2\innerprod{x-c_1}{x-c_2}+\delta^2-(\rho_1^2+\rho_2^2)
\end{align*}
so clearly (a),(b)\(\iff\)(d).\qedhere
\end{itemize}
\end{proof}

\begin{exer}[6]
\emph{Characterization of similitudes:}
\begin{itemize}[itemsep=0pt]
\item If \(u:E\to E\) is a bijective function such that \(\innerprod{u(x)}{u(y)}=\alpha\innerprod{x}{y}\) for all \(x,y\in E\) (\(\alpha>0\)), then \(u\)~is linear and consequently \(u\in\GO(E)\).
\item If \(u:E\to E\) is a bijective function such that \(d(u(x),u(y))=\alpha d(x,y)\) for all \(x,y\in E\) (\(\alpha>0\)), then \(u\)~is affine and consequently \(u\in\Sm(E)\).
\item If \(u\in\GL(E)\) is such that \(\innerprod{x}{y}=0\) implies \(\innerprod{u(x)}{u(y)}=0\) (in other words, \(u\)~preserves orthogonality), then \(u\in\GO(E)\).
\end{itemize}
\begin{proof}\
\begin{itemize}[itemsep=0pt]
\item By direct computation,
\begin{multline*}
\innerprod{u(x+y)-u(x)-u(y)}{u(x+y)-u(x)-u(y)}=\\\alpha\innerprod{x+y-x-y}{x+y-x-y}=0
\end{multline*}
so \(u(x+y)-u(x)-u(y)=0\) by positive definiteness of the inner product. Similarly \(u(\xi x)-\xi u(x)=0\). Therefore \(u\)~is linear.
\item Let \(v=t_{-u(0)}u\). Then \(v:E\to E\) is a bijective function and (5.1.1.5)
\begin{align*}
2\innerprod{v(x)}{v(y)}&=\norm{v(x)}^2+\norm{v(y)}^2-\norm{v(x)-v(y)}^2\\
	&=\norm{u(x)-u(0)}^2+\norm{u(y)-u(0)}^2-\norm{u(x)-u(y)}^2\\
	&=\alpha^2(\norm{x}^2+\norm{y}^2-\norm{x-y}^2)\\
	&=2\alpha^2\innerprod{x}{y}
\end{align*}
Therefore \(v\in\GO(E)\) by the previous item and \(u\in\Sm(E)\) (5.1.14).
\item First observe that \(\innerprod{u(x)}{u(y)}=0\) implies \(\innerprod{x}{y}=0\). Indeed, if \(y=0\) this is trivial. If \(y\ne 0\), then \(x=\xi y+z\) with \(\innerprod{z}{y}=0\) (5.1.7) and
\begin{align*}
0&=\innerprod{u(x)}{u(y)}\\
	&=\innerprod{u(\xi y+z)}{u(y)}&&\\
	&=\innerprod{\xi u(y)+u(z)}{u(y)}&&\\
	&=\xi\norm{u(y)}^2&&\text{since }\innerprod{u(z)}{u(y)}=0
\end{align*}
Since \(u(y)\ne 0\), this implies \(\xi=0\), so \(x=z\) and \(\innerprod{x}{y}=0\).

Now if \(y\ne 0\), it follows that \(\innerprod{u(x)}{u(y)}=0\) is another equation of the hyperplane \(\innerprod{x}{y}=0\) orthogonal to~\(y\), so (3.3.6) there is \(\mu_y>0\) with
\[\innerprod{u(x)}{u(y)}=\mu_y\innerprod{x}{y}\]
for all \(x\in E\). We claim that \(\mu_y\)~is independent of~\(y\). Indeed, if \(y'=0\) then trivially \(\innerprod{u(x)}{u(y')}=\mu_y\innerprod{x}{y'}\) for all \(x\in E\). If \(\innerprod{y}{y'}\ne 0\), then
\[\mu_{y}=\frac{\innerprod{u(y)}{u(y')}}{\innerprod{y}{y'}}=\mu_{y'}\]
Finally, if \(y'\ne 0\) and \(\innerprod{y}{y'}=0\), then \(\innerprod{y}{y+y'}\ne 0\) and \(\innerprod{y'}{y+y'}\ne 0\), so
\[\mu_y=\mu_{y+y'}=\mu_{y'}\]
by the previous case.\qedhere
\end{itemize}
\end{proof}
\end{exer}

\begin{exer}[7]
In~\(\GL(E)\), the normalizer of~\(\O(E)\) is~\(\GO(E)\).
\end{exer}
\begin{proof}
The normalizer contains~\(\GO(E)\) since if \(v\in\GO(E)\) and \(u\in\O(E)\), then \(\mu(vu\inv{v})=1\) (5.1.12.3), so \(vu\inv{v}\in\O(E)\).

Conversely, suppose \(v\in\GL(E)\) normalizes~\(\O(E)\). If \(\innerprod{x}{y}=0\) and \(x=0\), then trivially \(\innerprod{v(x)}{v(y)}=0\). If \(x\ne 0\), let \(u\in\O(E)\) be the symmetry about the hyperplane orthogonal to~\(x\), so \(u(x)=-x\) and \(u(y)=y\). Then \(vu\inv{v}\in\O(E)\) by hypothesis and \(vu\inv{v}v=vu\), so
\[\innerprod{v(x)}{v(y)}=\innerprod{v(u(x))}{v(u(y))}=-\innerprod{v(x)}{v(y)}\]
and hence \(\innerprod{v(x)}{v(y)}=0\). Since \(x,y\) were arbitrary, it follows that \(v\)~preserves orthogonality, and therefore \(v\in\GO(E)\) (Exercise~6).
\end{proof}

\subsection*{Section~2}
\begin{rmk}
In the plane, two lines are parallel if and only if there is a line to which they are both perpendicular; in this case, they are perpendicular to the same lines.
\end{rmk}
\begin{proof}
Parallel lines have the same direction, hence are perpendicular to the same lines, in particular the vector line orthogonal to that direction. Conversely, if two lines are both perpendicular to a third, then their direction is the vector line orthogonal to the direction of the third, hence they are parallel (5.2.2).
\end{proof}

\begin{rmk}
In the plane, a line is tangential to a circle at a point if and only if the line intersects the circle at the point and is perpendicular to the line passing through the point and the center of the circle.
\end{rmk}
\begin{proof}
The forward direction is proved in Exercise~5.1.4. The reverse direction follows from Pythagoras (5.1.5).
\end{proof}

\begin{rmk}
In~(5.2.3), the mapping \(f\mapsto a\) is actually a vector space \emph{isomorphism} from the dual space~\(\dual{E}\) to~\(E\). In fact, if \(\{a_1,a_2\}\)~is an orthonormal basis of~\(E\), then the isomorphism is just that determined by \(\dual{a_i}\mapsto a_i\), where \(\{\dual{a_1},\dual{a_2}\}\) is the dual basis (4.1.15).
\end{rmk}

\begin{rmk}
In~(5.2.4), we have vector space isomorphisms
\[\B(E,E;\R)\iso\Hom(E,\dual{E})\iso\End(E)\]
\end{rmk}

\begin{rmk}
In the complex plane~\(\C\), a number \(z\in\C\) induces an endomorphism through multiplication, and its conjugate~\(\conj{z}\) induces the adjoint endomorphism. Note \(\conj{\conj{z}}=z\) as in~(5.2.5.2), \(\conj{wz}=\conj{w}\ \conj{z}\) as in~(5.2.5.3), \(\conj{z}z=\abs{z}^2\) is the ``multiplicator'' of~\(z\) as in~(5.2.5.5), and \(z\)~is ``orthogonal'' if it is on the unit circle. If \(z\ne 0\), then \(z=\rho u\) with \(\rho=\abs{z}>0\) and \(u=z/\abs{z}\) on the unit circle uniquely determined, as in the polar decomposition~(5.1.12).

Endomorphisms of a plane can therefore be viewed as generalized complex numbers, with adjoints as generalized complex conjugates. This idea is made precise in~(5.5).
\end{rmk}

\begin{rmk}
In~(5.2.8), \(M(w)=(\alpha_{ij})\) with \(\alpha_{11}=\alpha_{22}=0\) and \(\alpha_{12}=-\alpha_{21}\) relative to an orthonormal basis (5.2.6), so \(\det(w)=\alpha_{12}^2\) (4.2.9.2). Since \(w\ne 0\), \(\det(w)>0\) and \(w\)~is invertible (4.2.8).
\end{rmk}

\begin{exer}[1]
If \(u\in\GL(E)\) is an involution, then \(u\in\O(E)\) if and only if \(u\)~is self-adjoint. If \(p\in\End(E)\) is idempotent, then \(p\)~is an orthogonal projection (5.1.6) if and only if \(p\)~is self-adjoint.
\end{exer}
\begin{proof}
By~(5.2.5.6), \(u\in\O(E)\) if and only if \(\adj{u}=\inv{u}=u\). By Exercise~3.2.1, \(p\)~is the projection onto~\(p(E)\) in the direction of~\(\kerz{p}\). If \(p(E)\)~is orthogonal to~\(\kerz{p}\), then
\begin{align*}
\innerprod{p(x)}{y}&=\innerprod{p(x)}{p(y)+(y-p(y))}&&\\
	&=\innerprod{p(x)}{p(y)}&&\text{since }\innerprod{p(x)}{y-p(y)}=0\\
	&=\innerprod{p(x)+(x-p(x))}{p(y)}&&\text{since }\innerprod{x-p(x)}{p(y)}=0\\
	&=\innerprod{x}{p(y)}
\end{align*}
for all \(x,y\in E\). Therefore \(\adj{p}=p\). Conversely if \(\adj{p}=p\), then for all \(x\in E\) and \(y\in\kerz{p}\),
\[\innerprod{p(x)}{y}=\innerprod{x}{p(y)}=\innerprod{x}{0}=0\]
so \(p(E)\)~is orthogonal to~\(\kerz{p}\).
\end{proof}

\begin{exer}[8]
Suppose \(u\in\End(E)\) has eigenvalues \(\lambda_1\ne\lambda_2\) with corresponding eigenvectors \(a_1,a_2\). If \(b_1\) (resp. \(b_2\)) is nonzero and orthogonal to~\(a_1\) (resp. \(a_2\)), then \(b_1\) (resp. \(b_2\)) is an eigenvector of~\(\adj{u}\) corresponding to~\(\lambda_2\) (resp. \(\lambda_1\)).
\end{exer}
\begin{proof}
Since \(\lambda_1\ne\lambda_2\), \(\{a_1,a_2\}\)~must be linearly independent and hence a basis. If \(\innerprod{a_1}{x}=\innerprod{a_1}{y}\) and \(\innerprod{a_2}{x}=\innerprod{a_2}{y}\), then for any \(z=\alpha_1a_1+\alpha_2a_2\),
\[\innerprod{z}{x}=\alpha_1\innerprod{a_1}{x}+\alpha_2\innerprod{a_2}{x}=\alpha_1\innerprod{a_1}{y}+\alpha_2\innerprod{a_2}{y}=\innerprod{z}{y}\]
and it follows that \(x=y\).

Now observe that for \(z=a_1,a_2\),
\[\innerprod{z}{\adj{u}(b_1)}=\innerprod{u(z)}{b_1}=\lambda_2\innerprod{z}{b_1}=\innerprod{z}{\lambda_2 b_1}\]
so \(\adj{u}(b_1)=\lambda_2b_1\). Similarly \(\adj{u}(b_2)=\lambda_1b_2\).
\end{proof}

\subsection*{Section~3}
\begin{rmk}
In~(5.3.1), if we fix an \emph{orientation} of the plane (4.3.1), then a similitude preserves orientation if it is direct and reverses orientation if it is inverse (4.3.2). In other words, \(\GOp(E)=\GO(E)\sect\GLp(E)\). If \(u\in\GO(E)\), then the following are equivalent:
\begin{itemize}[itemsep=0pt]
\item \(u\)~is a rotation
\item \(u\)~preserves length and orientation
\item \(u\)~preserves length and oriented area\footnote{By definition, \(u\)~preserves oriented area if \(u\in\SL(E)\).}
\item \(u\)~preserves orientation and oriented area
\end{itemize}
\end{rmk}

\begin{rmk}
By the polar decomposition (5.1.12) and~(5.3.2), an affine similitude of the plane is the composite of a translation after a linear scaling after a linear rotation or reflection. If the scaling is trivial, then the similitude is an affine isometry; otherwise, it is a linear similitude about another origin (5.3.6).
\end{rmk}

\begin{rmk}
We consider products of affine isometries \(u,v\in\Is(E)\) with \(u,v\ne 1\):
\begin{itemize}[itemsep=0pt]
\item If \(u\)~is a translation and \(v\)~is a rotation, then \(uv\)~is a rotation about another origin (5.3.7).
\item If \(u\)~is a translation and \(v\)~is a reflection, then \(uv\)~is a glide reflection (5.3.7).
\item If \(u\) and~\(v\) are rotations, then \(uv\)~is a translation or a rotation (5.3.7).
\item If \(u\)~is a rotation and \(v\)~is a reflection, then \(uv\)~is a glide reflection (5.3.7).
\item If \(u\) and~\(v\) are reflections in parallel lines, then \(uv\)~is a translation in a direction orthogonal to that of the lines (5.1.16).
\item If \(u\) and~\(v\) are reflections in non-parallel lines, then \(uv\)~is a rotation about the point of intersection of the lines. This follows from~(5.3.2) after taking the origin at the point of intersection.
\end{itemize}
\end{rmk}

\begin{exer}[2]
If \(u\in\End(E)\) is normal, then \(vu\inv{v}\)~is normal for all \(v\in\GO(E)\). Also, \(u\)~is self-adjoint or a direct similitude.
\end{exer}
\begin{proof}
By~(5.2.5.5), \(\adj{v}v=\alpha\mult 1\) for some \(\alpha>0\), so \(\adj{v}=\alpha\inv{v}\), \(\adj{(\inv{v})}=\alpha^{-1}v\), and
\begin{align*}
\adj{(vu\inv{v})}vu\inv{v}&=\adj{(\inv{v})}\adj{u}\adj{v}vu\inv{v}&&\\
	&=v\adj{u}\inv{v}vu\inv{v}&&\\
	&=v\adj{u}u\inv{v}&&\\
	&=vu\adj{u}\inv{v}&&\text{since \(u\)~is normal}\\
	&=vu\inv{v}v\adj{u}\inv{v}&&\\
	&=vu\inv{v}\adj{(vu\inv{v})}
\end{align*}
Therefore \(vu\inv{v}\)~is normal.

If \(M(u)=(\alpha_{ij})\) relative to some fixed orthonormal basis, then \(M(\adj{u})=(\alpha_{ji})\) (5.2.6.3) and
\[M(\adj{u})M(u)=M(\adj{u}u)=M(u\adj{u})=M(u)M(\adj{u})\]
implies \(\alpha_{12}=\pm\alpha_{21}\) and \(\alpha_{11}(\alpha_{12}-\alpha_{21})=\alpha_{22}(\alpha_{12}-\alpha_{21})\). If \(\alpha_{12}=\alpha_{21}\), then \(M(u)\)~is symmetric and \(u\)~is self-adjoint. If \(\alpha_{12}\ne\alpha_{21}\), then \(\alpha_{12}=-\alpha_{21}\) and \(\alpha_{11}=\alpha_{22}\), so
\[M(\adj{u}u)=(\alpha_{11}^2+\alpha_{12}^2)\mult 1=\det(u)\mult 1\]
Therefore \(\adj{u}u=\det(u)\mult 1\), so \(u\)~is a direct similitude.
\end{proof}

\begin{exer}[5]
Every affine isometry of the plane is the product of at most three reflections in lines. The product of two affine rotations is a translation if and only if the associated linear rotations are mutually inverse.
\end{exer}
\begin{proof}
If \(u\in\Is(E)\), then by cases (5.3.7):
\begin{itemize}[itemsep=0pt]
\item If \(u\)~is the identity, then \(u\)~is trivially the product of zero reflections.
\item If \(u\)~is a nonzero translation, then \(u\)~is the product of two reflections about lines perpendicular to the direction of translation (5.1.16).
\item If \(u\)~is a rotation, then \(u\)~is the product of two reflections about lines intersecting at the center of the rotation (5.3.2).
\item If \(u\)~is a glide reflection, then \(u\)~is a product of one or three reflections depending as the translation is zero or nonzero.
\end{itemize}
If \(u_1,u_2\in\Is(E)\) are affine rotations, then \(u_1=t_1v_1\) and \(u_2=t_2v_2\) where \(t_1,t_2\) are translations and \(v_1,v_2\in\Op(E)\). By (3.2.19.1), \(u_1u_2=tv_1v_2\), where \(t\)~is a translation, so \(u_1u_2\)~is a translation if and only if \(v_1v_2\)~is, which is true if and only if \(v_1v_2=1\).
\end{proof}

\subsection*{Section~4}
\begin{rmk}
In~(5.4.3), recall that \(\Psi(x,y)\)~is the \emph{oriented area} of the parallelogram determined by \(x\) and~\(y\) relative to the fixed orientation of the plane (see the remark on~(4.2.5) above). Therefore if \(u\)~is a rotation and \(x\)~is a unit vector, then \(\Psi(x,u(x))\)~is the signed distance from~\(u(x)\) to the line determined by~\(x\)---that is, the sine of the angle~\(\ang{x}{u(x)}\) of rotation.
\end{rmk}

\begin{rmk}
In~(5.4.10), a line~\(D'\) passing through~\(-a_1\) is tangential to~\(\U\) if and only if it is parallel to~\(D\), by remarks from~(5.2) above. Therefore \(D'\)~meets \(\U\) at a unique point~\(x\) distinct from~\(-a_1\) if and only if it meets~\(D\) at a unique point~\(y\).

For \(x\in\U'\), \(\innerprod{x}{x+a_1}=\innerprod{a_1}{x+a_1}\), so it follows from (5.1.7) and~(5.4.8) that the direction of~\(D_{-a_1,x}\) bisects \(\Delta_{0a_1}\) and~\(\Delta_{0x}\) and \(\ang{a_1}{x}=2\ang{a_1}{x+a_1}\). It is then immediate that \(x=\cos 2\theta\mult a_1+\sin 2\theta\mult a_2\) and \(y=\tan\theta\mult a_2\), where \(\theta=\ang{a_1}{x+a_1}\).
\end{rmk}

\begin{rmk}
In~(5.4.13), where \(\theta',\theta''\) correspond to~\(\Delta',\Delta''\), we have \(\theta'=\ang{-\Delta_0}{\Delta'}\) and \(\theta''=\ang{-\Delta_0}{\Delta''}\) (5.4.12), so
\[\theta''-\theta'=-\theta'+\theta''=\ang{\Delta'}{-\Delta_0}+\ang{-\Delta_0}{\Delta''}=\ang{\Delta'}{\Delta''}\]
Let \(x',x''\) be unit vectors in \(\Delta',\Delta''\). Then by (4.3.5) and~(5.4.9), \(\osector(\Delta',\Delta'')\)~is minor if and only if
\[\sin(\theta''-\theta')=\sin\ang{\Delta'}{\Delta''}=\sin\ang{x'}{x''}=\Psi(x',x'')>0\]
which is equivalent to \(\theta''-\theta'>0\) (5.4.12). Similarly, \(\osector(\Delta',\Delta'')\)~is major if and only if \(\theta''-\theta'<0\).
\end{rmk}

\begin{exer}[7]
In the set of pairs of vector half-lines (resp. vector lines), the orbits under the action of~\(\Op(E)\) are the sets of pairs with the same angle; under~\(\O(E)\), they are the sets of pairs with the same or opposite angle.
\end{exer}
\begin{proof}
By (5.4.7) and~(5.3.2).
\end{proof}

\subsection*{Section~5}
\begin{exer}[3]
\emph{Cayley's parametric representation:} If \(z\in\C\) with \(\abs{z}=1\) and \(z\ne -1\), then there exists \(\xi\in\R\) unique such that
\[z=(1+\xi i)(1-\xi i)^{-1}\]
\end{exer}
\begin{proof}
Let \(\theta=\arg z\), \(\xi=\tan(\theta/2)\), and \(w=1+\xi i\). By~(5.4.5.3),
\begin{align*}
w\conj{w}^{-1}&=\frac{w^2}{\abs{w}^2}\\
	&=\frac{(1+\xi i)^2}{1+\xi^2}\\
	&=\frac{1-\xi^2}{1+\xi^2}+\frac{2\xi}{1+\xi^2}i\\
	&=\cos\theta+i\sin\theta=z
\end{align*}
If \(z=(1+\xi' i)(1-\xi' i)^{-1}\), direct computation shows \(\xi'=\xi\).
\end{proof}

\section*{Chapter~VI}
\subsection*{Section~2}
\begin{rmk}
If \(\Phi\ne0\) is an alternating \emph{trilinear} form on~\(E\) and \(\Psi\)~is an alternating \emph{bilinear} form on~\(E\), then there is a unique vector \(v\in E\) with
\[\Psi(x,y)=\Phi(x,y,v)\tag{1}\]
for all vectors \(x,y\in E\).
\end{rmk}
\begin{proof}
Define \(\varphi:E^3\to E\) by
\[\varphi(x,y,z)=\Psi(y,z)x-\Psi(x,z)y+\Psi(x,y)z\tag{2}\]
Then \(\varphi\)~is alternating and trilinear, so there is a vector~\(v\) with \(\varphi=\Phi\mult v\) (6.2.2.1). If \(x,y\) are linearly dependent, then both sides of~(1) are zero. Otherwise there is~\(z\) with \(\Phi(x,y,z)=1\) and \(\varphi(x,y,z)=v\). Now by~(2),
\begin{align*}
\Phi(x,y,v)&=\Phi(x,y,\varphi(x,y,z))\\
	&=\Psi(y,z)\Phi(x,y,x)-\Psi(x,z)\Phi(x,y,y)+\Psi(x,y)\Phi(x,y,z)\\
	&=\Psi(x,y)
\end{align*}
The vector~\(v\) is uniquely determined since \(\Phi\ne0\).
\end{proof}
\noindent This shows that any function measuring oriented area in planes in~\(E\) actually measures oriented volume relative to a fixed vector.

\begin{exer}[11]
Let \(\Psi\ne0\) be an alternating bilinear form on~\(E\) and
\[N=\{\,x\in E\mid \Psi(x,y)=0\text{ for all }y\in E\,\}\]
Then \(N\ne 0\) and there is a basis of~\(E\) relative to which
\[M(\Psi)=\begin{pmatrix}
0&-1&0\\
1&0&0\\
0&0&0
\end{pmatrix}\]
\end{exer}
\begin{proof}
Let \(\Phi\ne0\) be an alternating trilinear form on~\(E\). By the remark above, there is a unique vector~\(c\) with
\[\Psi(x,y)=\Phi(x,y,c)\]
for all vectors \(x,y\). Clearly \(N=D_{0c}\ne0\), and if \(a,b\) are chosen so \(\Phi(a,b,c)=-1\), then \(M(\Psi)\)~has the desired form relative to the basis \(a,b,c\).
\end{proof}

\section*{Chapter~VII}
\subsection*{Section~1}
\begin{exer}[1]
Let \(a_1,a_2,a_3\) be an orthonormal basis of~\(E\), \(x_1,x_2,x_3\) vectors in~\(E\), and \(u\)~the endomorphism of~\(E\) with \(u(a_i)=x_i\). Define
\[G(x_1,x_2,x_3)=\begin{vmatrix}
\innerprod{x_1}{x_1}&\innerprod{x_1}{x_2}&\innerprod{x_1}{x_3}\\
\innerprod{x_2}{x_1}&\innerprod{x_2}{x_2}&\innerprod{x_2}{x_3}\\
\innerprod{x_3}{x_1}&\innerprod{x_3}{x_2}&\innerprod{x_3}{x_3}
\end{vmatrix}\]
Then
\[G(x_1,x_2,x_3)=\det(u)^2=\Psi(x_1,x_2,x_3)^2\ge0\tag{1}\]
where \(\Psi\)~is as in~(7.1.7).

Similarly define
\[G(x_1,x_2)=\begin{vmatrix}
\innerprod{x_1}{x_1}&\innerprod{x_1}{x_2}\\
\innerprod{x_2}{x_1}&\innerprod{x_2}{x_2}\\
\end{vmatrix}\]
Then
\[G(x_1,x_2)=\norm{x_1\cross x_2}^2\tag{2}\]
\end{exer}
\begin{proof}
Since
\[\innerprod{x_i}{x_j}=\innerprod{u(a_i)}{u(a_j)}=\innerprod{a_i}{\adj{u}u(a_j)}\]
it follows from~(7.1.1.4) that
\[G(x_1,x_2,x_3)=\det(\adj{u}u)=\det(\adj{u})\det(u)=\det(u)^2\]
On the other hand
\[\det(u)=\Psi(u(a_1),u(a_2),u(a_3))=\Psi(x_1,x_2,x_3)\]
which establishes~(1).

If \(x_1,x_2\) are linearly dependent, then both sides of~(2) are zero. If \(x_1,x_2\) are linearly independent, let
\[x_3=\frac{x_1\cross x_2}{\norm{x_1\cross x_2}}\]
Then
\[G(x_1,x_2)=G(x_1,x_2,x_3)=\Psi(x_1,x_2,x_3)^2=\norm{x_1\cross x_2}^2\qedhere\]
\end{proof}

\begin{exer}[3]
If \(u\in\GL(E)\), then
\[\adj{u}(x\cross y)=\det(u)\mult\inv{u}(x)\cross\inv{u}(y)\tag{1}\]
or equivalently
\[\inv{u}(x\cross y)=(\det(u))^{-1}\mult\adj{u}(x)\cross\adj{u}(y)\tag{2}\]
Moreover
\[\adj{u}(u(x)\cross u(y))=\det(u)\mult x\cross y=u(\adj{u}(x)\cross\adj{u}(y))\tag{3}\]
\end{exer}
\begin{proof}
Let~\(\Psi\) be as in~(7.1.7). For any vector~\(z\),
\begin{align*}
\innerprod{\adj{u}(x\cross y)}{z}&=\innerprod{x\cross y}{u(z)}\\
	&=\Psi(x,y,u(z))\\
	&=\Psi(u(\inv{u}(x)),u(\inv{u}(y)),u(z))\\
	&=\det(u)\mult\Psi(\inv{u}(x),\inv{u}(y),z)\\
	&=\det(u)\mult\innerprod{\inv{u}(x)\cross\inv{u}(y)}{z}\\
	&=\innerprod{\det(u)\mult\inv{u}(x)\cross\inv{u}(y)}{z}
\end{align*}
Now (1)~follows from definiteness of the inner product.

Substituting \(\adj{u}\) for~\(u\) in~(1) and noting that \(\adj{(\adj{u})}=u\) and \(\det(\adj{u})=\det(u)\), we obtain
\[u(x\cross y)=\det(u)\mult\inv{(\adj{u})}(x)\cross\inv{(\adj{u})}(y)\tag{4}\]
Now substituting \(\inv{u}\) for~\(u\) in~(4) and noting that \(\adj{(\inv{u})}=\inv{(\adj{u})}\), we obtain~(2). By reversing these steps, we obtain (1) from~(2).

Finally, (3)~follows by substituting \(u(x)\) for~\(x\) and \(u(y)\) for~\(y\) in (1), and by substituting \(\adj{u}(x)\) for~\(x\) and \(\adj{u}(y)\) for~\(y\) in~(4). Note that (3)~is \emph{self-dual}, meaning it is invariant under substitution of \(\adj{u}\) for~\(u\).
\end{proof}

% References
\begin{thebibliography}{0}
\bibitem{dieudonne} Dieudonn\'e, J. \textit{Linear Algebra and Geometry.} Hermann, 1969.
\end{thebibliography}
\end{document}
